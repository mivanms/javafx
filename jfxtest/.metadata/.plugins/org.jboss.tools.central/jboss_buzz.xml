<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" media="screen" href="/~d/styles/atom10full.xsl"?><?xml-stylesheet type="text/css" media="screen" href="http://feeds.feedburner.com/~d/styles/itemcontent.css"?><feed xmlns="http://www.w3.org/2005/Atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:feedburner="http://rssnamespace.org/feedburner/ext/1.0"><title>JBoss Tools Aggregated Feed</title><link rel="alternate" href="http://tools.jboss.org" /><subtitle>JBoss Tools Aggregated Feed</subtitle><dc:creator>JBoss Tools</dc:creator><atom10:link xmlns:atom10="http://www.w3.org/2005/Atom" rel="self" type="application/atom+xml" href="http://feeds.feedburner.com/jbossbuzz" /><feedburner:info uri="jbossbuzz" /><atom10:link xmlns:atom10="http://www.w3.org/2005/Atom" rel="hub" href="http://pubsubhubbub.appspot.com/" /><entry><title>Using Ansible Galaxy Roles in Ansible Playbook Bundles</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/KhvDAScZbGM/" /><category term="ansible" scheme="searchisko:content:tags" /><category term="apb" scheme="searchisko:content:tags" /><category term="Cloud Automation" scheme="searchisko:content:tags" /><category term="Cloud Services" scheme="searchisko:content:tags" /><category term="feed_group_name_nonmiddleware" scheme="searchisko:content:tags" /><category term="feed_name_redhat_developer_blog" scheme="searchisko:content:tags" /><category term="Open Service Broker" scheme="searchisko:content:tags" /><category term="OpenShift Enterprise by Red Hat" scheme="searchisko:content:tags" /><category term="Red Hat OpenShift" scheme="searchisko:content:tags" /><category term="Red Hat OpenShift Container Platform" scheme="searchisko:content:tags" /><author><name>Siamak Sadeghianfar</name></author><id>searchisko:content:id:jbossorg_blog-using_ansible_galaxy_roles_in_ansible_playbook_bundles</id><updated>2018-05-22T18:14:52Z</updated><published>2018-05-22T18:14:52Z</published><content type="html">&lt;p&gt;[In case you aren&amp;#8217;t following the &lt;a href="https://blog.openshift.com/"&gt;OpenShift blog&lt;/a&gt;, I&amp;#8217;m cross posting &lt;a href="https://blog.openshift.com/using-ansible-galaxy-roles-in-ansible-playbook-bundles"&gt;my article&lt;/a&gt; here because I think it will be of interest to the Red Hat Developer commnity.]&lt;/p&gt; &lt;p&gt;The Open Service Broker API standard aims to standardize how services (cloud, third-party, on-premise, legacy, etc) are delivered to applications running on cloud platforms like OpenShift. This allows applications to consume services the exact same way no matter on which cloud platform they are deployed. The service broker pluggable architecture enables admins to add third-party brokers to the platform in order to make third-party and cloud services available to the application developers directly from the OpenShift service catalog. As an example &lt;a href="https://aws.amazon.com/partners/servicebroker/"&gt;AWS Service Broker&lt;/a&gt; created jointly by Amazon and Red Hat, &lt;a href="https://github.com/azure/open-service-broker-azure"&gt;Azure Service Broker&lt;/a&gt; created by Microsoft and &lt;a href="https://github.com/google/helm-broker"&gt;Helm Service Broker&lt;/a&gt; created by Google to allow consumption of AWS services, Azure services and Helm charts on Kubernetes and OpenShift. Furthermore, admins can &lt;a href="https://github.com/openshift/open-service-broker-sdk"&gt;create their own brokers&lt;/a&gt; in order to make custom services like provisioning an Oracle database on their internal Oracle RAC available to the developers through the service catalog.&lt;span id="more-496237"&gt;&lt;/span&gt;&lt;/p&gt; &lt;p&gt;&lt;img class=" aligncenter size-full wp-image-496277 " data-add-featherlight="https://developers.redhat.com/blog/wp-content/uploads/2018/05/service-broker-1.png" src="https://developers.redhat.com/blog/wp-content/uploads/2018/05/service-broker-1.png" alt="" srcset="https://developers.redhat.com/blog/wp-content/uploads/2018/05/service-broker-1.png 940w, https://developers.redhat.com/blog/wp-content/uploads/2018/05/service-broker-1-300x144.png 300w, https://developers.redhat.com/blog/wp-content/uploads/2018/05/service-broker-1-768x368.png 768w" sizes="(max-width: 940px) 100vw, 940px" /&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="http://automationbroker.io/"&gt;OpenShift Automation Broker&lt;/a&gt; is a service broker that is included in OpenShift out-of-the-box and leverages a lightweight, container-based application definition called an Ansible Playbook Bundle (APB) to automate service provisioning using Ansible. The playbooks can perform actions on OpenShift platform, as well as off platform such as provisioning a virtual machine on VMware and installing a database on it.&lt;/p&gt; &lt;h3&gt;Ansible Galaxy Roles&lt;/h3&gt; &lt;p&gt;The &lt;a href="https://docs.openshift.com/container-platform/3.9/apb_devel/writing/getting_started.html#apb-devel-writing-gs-creating"&gt;current structure of APBs&lt;/a&gt; requires all Ansible roles used in the playbooks to be available in the &lt;code&gt;roles&lt;/code&gt; directory, as Ansible normally demands it.&lt;/p&gt; &lt;pre&gt;myapb ├── apb.yml ├── Dockerfile ├── playbooks │ ├── deprovision.yml │ └── provision.yml └── roles ├── myrole1 └── myrole2 &lt;/pre&gt; &lt;p&gt;While that is useful, the power of Ansible lies in the large ecosystem of Ansible roles that are created by third-parties and the community which simplifies automating virtually anything. The expression “there is role for that!” is not far from reality in the Ansible world.&lt;/p&gt; &lt;p&gt;&lt;a href="https://galaxy.ansible.com"&gt;Ansible Galaxy&lt;/a&gt; is the hub for finding, reusing and sharing Ansible roles. Whenever one needs to create an Ansible role, the first step is usually to search on Ansible Galaxy for a role that already does the task needed.&lt;/p&gt; &lt;p&gt;Although on the roadmap, currently the Ansible Galaxy roles cannot be directly used in the APB playbooks unless they are already downloaded in the roles directory. Nevertheless, you can still use Ansible Galaxy roles in the APB through a few extra steps which are explained in the next section.&lt;/p&gt; &lt;h3&gt;Using Ansible Galaxy Roles in APBs&lt;/h3&gt; &lt;p&gt;The first step for using the Ansible Galaxy roles is to list those roles as dependencies in a &lt;code&gt;requirements.yml&lt;/code&gt; file in the root of the APB which is the standard way of describing the Ansible Galaxy roles in Ansible:&lt;/p&gt; &lt;pre&gt;- src: siamaksade.openshift_sonatype_nexus version: ocp-3.9 - src: siamaksade.openshift_gogs version: ocp-3.9 - src: siamaksade.openshift_eclipse_che version: ocp-3.9-1 &lt;/pre&gt; &lt;p&gt;The APB directory structure would look like the following:&lt;/p&gt; &lt;pre&gt;myapb/ ├── apb.yml ├── Dockerfile ├── requirements.yml ├── playbooks │ ├── deprovision.yml │ └── provision.yml └── roles ├── myrole1 └── myrole2 &lt;/pre&gt; &lt;p&gt;The above &lt;code&gt;requirements.yml&lt;/code&gt; file declares that this APB requires the &lt;code&gt;openshift_sonatype_nexus&lt;/code&gt;, &lt;code&gt;openshift_gogs&lt;/code&gt; and &lt;code&gt;openshift_eclipse_che&lt;/code&gt; roles from Ansible Galaxy.&lt;/p&gt; &lt;p&gt;Since Ansible Galaxy is not integrated into APBs yet, the next step is to add a few lines to the APB &lt;code&gt;Dockerfile&lt;/code&gt; to install the dependencies when the APB is being built and packaged:&lt;/p&gt; &lt;pre&gt;ADD requirements.yml /opt/apb/actions/requirements.yml RUN ansible-galaxy install -r /opt/apb/actions/requirements.yml &lt;/pre&gt; &lt;p&gt;Now you can use the declared roles in your APB action playbooks, for example in the &lt;code&gt;playbooks/provision.yml&lt;/code&gt; which is the playbook that runs when an APB is provisioned via the OpenShift service catalog:&lt;/p&gt; &lt;pre&gt; roles: - role: ansible.kubernetes-modules install_python_requirements: no - role: ansibleplaybookbundle.asb-modules - role: siamaksade.openshift_sonatype_nexus - role: siamaksade.openshift_gogs - role: siamaksade.openshift_eclipse_che &lt;/pre&gt; &lt;h3&gt;Further Integration With Ansible Galaxy&lt;/h3&gt; &lt;p&gt;In OpenShift 3.11, APBs will become a first class citizen of Ansible Galaxy and can be shared and reused conveniently the very same way that Ansible roles are shared and reused in Ansible Galaxy. The OpenShift Automation Broker will be able to discover APBs directly from Ansible Galaxy which means that a developer can publish their APB source code to Ansible Galaxy and the OpenShift Automation Broker will run the source directly on a special APB base image. This will allow developers to test their source code changes without the hassle of rebuilding the full APB container images each time they want to test it.&lt;/p&gt; &lt;p&gt;Furthermore, the integration with Ansible Galaxy would allow automatic resolution of the role dependencies based on the APB metadata and would allow using any role that is published to Ansible Galaxy and declared as a dependency in the metadata, directly in the playbooks.&lt;/p&gt; &lt;h3&gt;Summary&lt;/h3&gt; &lt;p&gt;APBs enable using Ansible playbooks for automating provisioning of services on OpenShift and also other platforms. Ansible Galaxy enriches the Ansible experience by providing a large set of pre-built roles that can be used to automate virtually anything. By creating a &lt;code&gt;requirements.yml&lt;/code&gt; file and modifying the &lt;code&gt;Dockerfile&lt;/code&gt; in an APB, one can take advantage of the Ansible Galaxy roles in authoring APBs and build complex automation flows.&lt;/p&gt; &lt;p&gt;Ansible Galaxy roles play an important role in APB authoring and therefore native Ansible Galaxy support in APBs is planned for future releases of OpenShift, possibly in OpenShift 3.11.&lt;/p&gt; &lt;p&gt;&lt;a class="a2a_button_facebook" href="https://www.addtoany.com/add_to/facebook?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2018%2F05%2F22%2Fusing-ansible-galaxy-roles-in-ansible-playbook-bundles%2F&amp;#38;linkname=Using%20Ansible%20Galaxy%20Roles%20in%20Ansible%20Playbook%20Bundles" title="Facebook" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_twitter" href="https://www.addtoany.com/add_to/twitter?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2018%2F05%2F22%2Fusing-ansible-galaxy-roles-in-ansible-playbook-bundles%2F&amp;#38;linkname=Using%20Ansible%20Galaxy%20Roles%20in%20Ansible%20Playbook%20Bundles" title="Twitter" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_google_plus" href="https://www.addtoany.com/add_to/google_plus?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2018%2F05%2F22%2Fusing-ansible-galaxy-roles-in-ansible-playbook-bundles%2F&amp;#38;linkname=Using%20Ansible%20Galaxy%20Roles%20in%20Ansible%20Playbook%20Bundles" title="Google+" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_linkedin" href="https://www.addtoany.com/add_to/linkedin?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2018%2F05%2F22%2Fusing-ansible-galaxy-roles-in-ansible-playbook-bundles%2F&amp;#38;linkname=Using%20Ansible%20Galaxy%20Roles%20in%20Ansible%20Playbook%20Bundles" title="LinkedIn" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_email" href="https://www.addtoany.com/add_to/email?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2018%2F05%2F22%2Fusing-ansible-galaxy-roles-in-ansible-playbook-bundles%2F&amp;#38;linkname=Using%20Ansible%20Galaxy%20Roles%20in%20Ansible%20Playbook%20Bundles" title="Email" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_print" href="https://www.addtoany.com/add_to/print?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2018%2F05%2F22%2Fusing-ansible-galaxy-roles-in-ansible-playbook-bundles%2F&amp;#38;linkname=Using%20Ansible%20Galaxy%20Roles%20in%20Ansible%20Playbook%20Bundles" title="Print" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_reddit" href="https://www.addtoany.com/add_to/reddit?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2018%2F05%2F22%2Fusing-ansible-galaxy-roles-in-ansible-playbook-bundles%2F&amp;#38;linkname=Using%20Ansible%20Galaxy%20Roles%20in%20Ansible%20Playbook%20Bundles" title="Reddit" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_flipboard" href="https://www.addtoany.com/add_to/flipboard?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2018%2F05%2F22%2Fusing-ansible-galaxy-roles-in-ansible-playbook-bundles%2F&amp;#38;linkname=Using%20Ansible%20Galaxy%20Roles%20in%20Ansible%20Playbook%20Bundles" title="Flipboard" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_dd addtoany_share_save addtoany_share" href="https://www.addtoany.com/share#url=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2018%2F05%2F22%2Fusing-ansible-galaxy-roles-in-ansible-playbook-bundles%2F&amp;#38;title=Using%20Ansible%20Galaxy%20Roles%20in%20Ansible%20Playbook%20Bundles" data-a2a-url="https://developers.redhat.com/blog/2018/05/22/using-ansible-galaxy-roles-in-ansible-playbook-bundles/" data-a2a-title="Using Ansible Galaxy Roles in Ansible Playbook Bundles"&gt;&lt;img src="https://static.addtoany.com/buttons/favicon.png" alt="Share"&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;The post &lt;a rel="nofollow" href="https://developers.redhat.com/blog/2018/05/22/using-ansible-galaxy-roles-in-ansible-playbook-bundles/"&gt;Using Ansible Galaxy Roles in Ansible Playbook Bundles&lt;/a&gt; appeared first on &lt;a rel="nofollow" href="https://developers.redhat.com/blog"&gt;RHD Blog&lt;/a&gt;.&lt;/p&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/KhvDAScZbGM" height="1" width="1" alt=""/&gt;</content><summary>[In case you aren’t following the OpenShift blog, I’m cross posting my article here because I think it will be of interest to the Red Hat Developer commnity.] The Open Service Broker API standard aims to standardize how services (cloud, third-party, on-premise, legacy, etc) are delivered to applications running on cloud platforms like OpenShift. This allows applications to consume services the exa...</summary><dc:creator>Siamak Sadeghianfar</dc:creator><dc:date>2018-05-22T18:14:52Z</dc:date><feedburner:origLink>https://developers.redhat.com/blog/2018/05/22/using-ansible-galaxy-roles-in-ansible-playbook-bundles/</feedburner:origLink></entry><entry><title>Teiid 10.3.1 Released</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/kVXg8DUsmaw/teiid-1031-released.html" /><category term="feed_group_name_teiid" scheme="searchisko:content:tags" /><category term="feed_name_teiid" scheme="searchisko:content:tags" /><author><name>Steven Hawkins</name></author><id>searchisko:content:id:jbossorg_blog-teiid_10_3_1_released</id><updated>2018-05-22T14:27:01Z</updated><published>2018-05-22T14:27:00Z</published><content type="html">We are pleased to announce the release of &lt;a href="https://teiid.github.io/teiid.io/teiid_four_ways/teiid_wildfly/downloads/"&gt;Teiid 10.3.1&lt;/a&gt; (A regression with TEIID-5314 caused an immediate patch release).&amp;nbsp; See all &lt;a href="https://issues.jboss.org/projects/TEIID/versions/12337178"&gt;48 issues&lt;/a&gt; addressed.&amp;nbsp; The feature highlight are:&lt;br /&gt;&lt;ul&gt;&lt;li&gt;&lt;a href="https://issues.jboss.org/browse/TEIID-5293"&gt;TEIID-5293&lt;/a&gt; Added implicit partition wise joining in non-multisource scenarios as well.&lt;/li&gt;&lt;li&gt;&lt;a href="https://issues.jboss.org/browse/TEIID-5308"&gt;TEIID-5308&lt;/a&gt; Added ENV_VAR and SYS_PROP functions and clarified the usage of the ENV function.&lt;/li&gt;&lt;li&gt;&lt;a href="https://issues.jboss.org/browse/TEIID-4745"&gt;TEIID-4745&lt;/a&gt; Added a polling query to trigger the reload of materialized views.&lt;/li&gt;&lt;li&gt;&lt;a href="https://issues.jboss.org/browse/TEIID-5317"&gt;TEIID-5317&lt;/a&gt; Added ODBC 3.0 functions current_time, current_date, current_timestamp.&lt;/li&gt;&lt;li&gt;&lt;a href="https://issues.jboss.org/browse/TEIID-5314"&gt;TEIID-5314&lt;/a&gt; Environment variables can now be used instead of or to override system properties.&lt;/li&gt;&lt;li&gt;&lt;a href="https://issues.jboss.org/browse/TEIID-5307"&gt;TEIID-5307&lt;/a&gt; Added more information_schema tables for pg compatibility.&lt;/li&gt;&lt;li&gt;&lt;a href="https://issues.jboss.org/browse/TEIID-4864"&gt;TEIID-4864&lt;/a&gt; Added update support to the Excel translator.&lt;/li&gt;&lt;/ul&gt;Special thanks to the many community members who contributed to this release - in particular Andreas Krück, Divyesh Vallabh, Mathieu Rampant, Mike Higgins, Pedro Inácio, Yuming Zhu, Michael Echevarria, and Bram Gadeyne.&lt;br /&gt;&lt;br /&gt;We should also extend a warm welcome to&amp;nbsp;&lt;a href="https://github.com/rkorytkowski"&gt;Rafal Korytkowski&lt;/a&gt; to the Teiid Team. Rafal has begun by finishing off the work on the &lt;a href="https://issues.jboss.org/browse/TEIID-4520"&gt;Exosal translator&lt;/a&gt;&amp;nbsp;and will help with efforts related to cloud and OpenShift support.&lt;br /&gt;&lt;br /&gt;Other Important Stuff:&lt;br /&gt;&lt;ul&gt;&lt;li&gt;The new website is (mostly) live! See &lt;a href="https://teiid.io/"&gt;teiid.io&lt;/a&gt;. There are no under construction animated GIFs, but you can expect a few issues as we get the new Hugo/Netlify/Travis build worked out. Notice that we're effectively combining tooling and backend under a single site.&amp;nbsp; The content is being updated to be more relevant for cloud and OpenShift deployments. A lot of work is happening in creating a &lt;a href="https://github.com/teiid/beetle-studio"&gt;self-service data virtualization OpenShift based solution&lt;/a&gt;. If you have any interest in becoming involved, or would like to see more around the &lt;a href="https://github.com/teiid/teiid-komodo/"&gt;image build/configuration&lt;/a&gt; please let us know.&lt;/li&gt;&lt;li&gt;Teiid 10.2.2 and 10.1.4 will be available in the next couple of days. That will effectively end support for the 10.1.x release stream.&lt;/li&gt;&lt;/ul&gt;&lt;div&gt;Thank you,&lt;/div&gt;&lt;div&gt;The Teiid Team&lt;/div&gt;&lt;br /&gt;&lt;br /&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/kVXg8DUsmaw" height="1" width="1" alt=""/&gt;</content><summary>We are pleased to announce the release of Teiid 10.3.1 (A regression with TEIID-5314 caused an immediate patch release).  See all 48 issues addressed.  The feature highlight are: TEIID-5293 Added implicit partition wise joining in non-multisource scenarios as well. TEIID-5308 Added ENV_VAR and SYS_PROP functions and clarified the usage of the ENV function. TEIID-4745 Added a polling query to trigg...</summary><dc:creator>Steven Hawkins</dc:creator><dc:date>2018-05-22T14:27:00Z</dc:date><feedburner:origLink>http://teiid.blogspot.com/2018/05/teiid-1031-released.html</feedburner:origLink></entry><entry><title>Teiid 10.1.4 Released</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/bjKDSYxJ9kw/teiid-1014-released.html" /><category term="feed_group_name_teiid" scheme="searchisko:content:tags" /><category term="feed_name_teiid" scheme="searchisko:content:tags" /><author><name>Steven Hawkins</name></author><id>searchisko:content:id:jbossorg_blog-teiid_10_1_4_released</id><updated>2018-05-22T12:19:46Z</updated><published>2018-05-22T12:19:00Z</published><content type="html">Teiid &lt;a href="http://teiid.jboss.org/downloads_10x/"&gt;10.1.4&lt;/a&gt; has been released. It resolves 12 issues: &lt;ul&gt;&lt;li&gt;[&lt;a href='https://issues.jboss.org/browse/TEIID-5336'&gt;TEIID-5336&lt;/a&gt;] - Improve TEIID-5253 &lt;/li&gt;&lt;li&gt;[&lt;a href='https://issues.jboss.org/browse/TEIID-4784'&gt;TEIID-4784&lt;/a&gt;] - Provide functionality to perform RENAME table in DDL scripts &lt;/li&gt;&lt;li&gt;[&lt;a href='https://issues.jboss.org/browse/TEIID-5319'&gt;TEIID-5319&lt;/a&gt;] - SAP IQ translator wrong pushdown of query with multiple JOINs &lt;/li&gt;&lt;li&gt;[&lt;a href='https://issues.jboss.org/browse/TEIID-5324'&gt;TEIID-5324&lt;/a&gt;] - MongoDB: SecurityType &amp;quot;None&amp;quot; is not working &lt;/li&gt;&lt;li&gt;[&lt;a href='https://issues.jboss.org/browse/TEIID-5326'&gt;TEIID-5326&lt;/a&gt;] - SAP IQ timestamp conversion to varchar wrong resulting format &lt;/li&gt;&lt;li&gt;[&lt;a href='https://issues.jboss.org/browse/TEIID-5328'&gt;TEIID-5328&lt;/a&gt;] - regression of org.teiid.padSpace does not affect to the &amp;quot;IN&amp;quot; operator behavior &lt;/li&gt;&lt;li&gt;[&lt;a href='https://issues.jboss.org/browse/TEIID-5329'&gt;TEIID-5329&lt;/a&gt;] - Problem with salesforce url &lt;/li&gt;&lt;li&gt;[&lt;a href='https://issues.jboss.org/browse/TEIID-5330'&gt;TEIID-5330&lt;/a&gt;] - FIRST_VALUE/LAST_VALUE/LEAD/LAG functions always try to return integer &lt;/li&gt;&lt;li&gt;[&lt;a href='https://issues.jboss.org/browse/TEIID-5331'&gt;TEIID-5331&lt;/a&gt;] - LEAD/LAG ignores ORDER BY in the OVER clause &lt;/li&gt;&lt;li&gt;[&lt;a href='https://issues.jboss.org/browse/TEIID-5333'&gt;TEIID-5333&lt;/a&gt;] - Complex foreign keys set the referenced key regardless of order &lt;/li&gt;&lt;li&gt;[&lt;a href='https://issues.jboss.org/browse/TEIID-5339'&gt;TEIID-5339&lt;/a&gt;] - Vertica join query fails due to unexpected ordering of intermediate results &lt;/li&gt;&lt;li&gt;[&lt;a href='https://issues.jboss.org/browse/TEIID-5342'&gt;TEIID-5342&lt;/a&gt;] - If excel FIRST_DATA_ROW_NUMBER is past all rows, the last row is still used &lt;/li&gt;&lt;/ul&gt; This concludes the community releases on 10.1.x. Please upgrade to 10.2 or 10.3. 10.2.2 will be released tomorrow. Thanks, Steve&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/bjKDSYxJ9kw" height="1" width="1" alt=""/&gt;</content><summary>Teiid 10.1.4 has been released. It resolves 12 issues: [TEIID-5336] - Improve TEIID-5253 [TEIID-4784] - Provide functionality to perform RENAME table in DDL scripts [TEIID-5319] - SAP IQ translator wrong pushdown of query with multiple JOINs [TEIID-5324] - MongoDB: SecurityType "None" is not working [TEIID-5326] - SAP IQ timestamp conversion to varchar wrong resulting format [TEIID-5328] - regress...</summary><dc:creator>Steven Hawkins</dc:creator><dc:date>2018-05-22T12:19:00Z</dc:date><feedburner:origLink>http://teiid.blogspot.com/2018/05/teiid-1014-released.html</feedburner:origLink></entry><entry><title>Container Testing in OpenShift with Meta Test Family</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/jERJEcGQw38/" /><category term="ci/cd" scheme="searchisko:content:tags" /><category term="Containers" scheme="searchisko:content:tags" /><category term="feed_group_name_nonmiddleware" scheme="searchisko:content:tags" /><category term="feed_name_redhat_developer_blog" scheme="searchisko:content:tags" /><category term="meta test family" scheme="searchisko:content:tags" /><category term="MTF" scheme="searchisko:content:tags" /><category term="Red Hat OpenShift" scheme="searchisko:content:tags" /><category term="Red Hat OpenShift Container Platform" scheme="searchisko:content:tags" /><category term="test automation" scheme="searchisko:content:tags" /><category term="testing" scheme="searchisko:content:tags" /><author><name>unknown</name></author><id>searchisko:content:id:jbossorg_blog-container_testing_in_openshift_with_meta_test_family</id><updated>2018-05-22T11:00:54Z</updated><published>2018-05-22T11:00:54Z</published><content type="html">&lt;p&gt;Without proper testing, we should not ship any container. We should guarantee that a given service in a container works properly. &lt;a href="https://github.com/fedora-modularity/meta-test-family"&gt;Meta Test Family&lt;/a&gt; (MTF) was designed for this very purpose.&lt;/p&gt; &lt;p&gt;Containers can be tested as “standalone” containers and as “orchestrated” containers. Let’s look at how to test containers with the Red Hat &lt;a href="https://www.openshift.com/"&gt;OpenShift&lt;/a&gt; environment. This article describes how to do that and what actions are needed.&lt;/p&gt; &lt;p&gt;MTF is a &lt;b&gt;minimalistic library&lt;/b&gt; built on the existing &lt;a href="https://avocado-framework.github.io/"&gt;Avocado&lt;/a&gt; and &lt;a href="https://github.com/behave/behave"&gt;behave&lt;/a&gt; testing frameworks, assisting developers in quickly enabling test automation and requirements. MTF adds basic support and abstraction for testing various module artifact types: RPM-based, Docker images, and more. For detailed information about the framework and how to use it check out the &lt;a href="http://meta-test-family.readthedocs.io/en/latest/"&gt;MTF documentation&lt;/a&gt;.&lt;span id="more-495577"&gt;&lt;/span&gt;&lt;/p&gt; &lt;h1&gt;Installing MTF&lt;/h1&gt; &lt;p&gt;Before you can start testing, install MTF from the official EPEL repository using &lt;code&gt;sudo&lt;/code&gt;:&lt;/p&gt; &lt;pre&gt;sudo yum install -y meta-test-family&lt;/pre&gt; &lt;p&gt;A COPR repository contains a development version of MTF that should not be used in a production environment.&lt;/p&gt; &lt;p&gt;However, you can install MTF with these commands:&lt;/p&gt; &lt;pre&gt;dnf copr enable phracek/meta-test-family dnf install -y meta-test-family&lt;/pre&gt; &lt;p&gt;To install MTF directly from GitHub, run these commands:&lt;/p&gt; &lt;pre&gt;git clone git@github.com:fedora-modularity/meta-test-family.git cd meta-test-family sudo python setup.py install&lt;/pre&gt; &lt;p&gt;Now, you can start testing containers in the OpenShift environment.&lt;/p&gt; &lt;h1&gt;Prepare a Test for OpenShift&lt;/h1&gt; &lt;p&gt;Running your containers locally is dead-simple: just use the &lt;code&gt;docker run&lt;/code&gt; command. But that’s not how you run your application in production—that’s OpenShift’s business. To make sure your containers are orchestrated well, you should test them in the same environment.&lt;/p&gt; &lt;p&gt;Bear in mind that standalone and orchestrated environments are different. Standalone containers can be executed easily with a single command. Managing such containers isn’t as easy: you need to figure out persistent storage, backups, updates, routing, and scaling—all the things you get for free with orchestrators.&lt;/p&gt; &lt;p&gt;The OpenShift environment has its own characteristics: security restrictions, differences in persistent storage logic, expectation of stateless pods, support for updates, a multi-node environment, native source-to-image support, and much much more.  Deploying an orchestrator here is not an easy task. This is the reason why MTF supports OpenShift: so you can easily test your containerized application in an orchestrated environment.&lt;/p&gt; &lt;p&gt;Before running and preparing the OpenShift environment, you have to create a test and a configuration file for MTF in YAML format. These two files have to be in the same directory, and tests will be executed from the directory.&lt;/p&gt; &lt;h1&gt;Structure of MTF Tests&lt;/h1&gt; &lt;p&gt;Create a directory which will contain the following files:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;code&gt;config.yaml&lt;/code&gt;: The configuration file for MTF&lt;/li&gt; &lt;li&gt;&lt;code&gt;sanity1.py&lt;/code&gt;: The container test that is run by MTF&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Configuration File for MTF&lt;/h1&gt; &lt;p&gt;The configuration file loos like this:&lt;/p&gt; &lt;pre&gt;document: modularity-testing version: 1 name: memcached service:     port: 11211 module:     openshift:            start: VARIABLE_MEMCACHED=60         container: docker.io/modularitycontainers/memcached &lt;/pre&gt; &lt;p&gt;Here’s an explanation of each field in the YAML config file for MTF:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;code&gt;service.port&lt;/code&gt;: Port where the service is available&lt;/li&gt; &lt;li&gt;&lt;code&gt;module.openshift&lt;/code&gt;: Configuration part relevant only for the OpenShift environment&lt;/li&gt; &lt;li&gt;&lt;code&gt;module.openshift.start&lt;/code&gt;:&lt;b&gt; &lt;/b&gt;Parameters that will be used for testing in OpenShift&lt;/li&gt; &lt;li&gt;&lt;code&gt;module.openshift.container&lt;/code&gt;: Reference to the container, which will be used for testing in OpenShift&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Test for memcached Container&lt;/h1&gt; &lt;p&gt;Here’s an example of a &lt;code&gt;memcached&lt;/code&gt; test for a container:&lt;/p&gt; &lt;pre&gt;$ cat memcached_sanity.py import pexpect from avocado import main from avocado.core import exceptions from moduleframework import module_framework from moduleframework import common class MemcachedSanityCheck(module_framework.AvocadoTest): """ :avocado: enable """ def test_smoke(self):    self.start()    session = pexpect.spawn("telnet %s %s " % (self.ip_address, self.getConfig()['service']['port']))    session.sendline('set Test 0 100 4\r\n\n')    session.sendline('JournalDev\r\n\n')    common.print_info("Expecting STORED")    session.expect('STORED')    common.print_info("STORED was catched")    session.close() if __name__ == '__main__':    main() &lt;/pre&gt; &lt;p&gt;This test connects to &lt;code&gt;memcached&lt;/code&gt; via telnet on the given IP address and port. The port is specified in the MTF configuration file. The following sections speak more about the IP address.&lt;/p&gt; &lt;h1&gt;Prepare OpenShift for Container Testing&lt;/h1&gt; &lt;p&gt;MTF can install the OpenShift environment on your local system with the &lt;code&gt;mtf-env-set&lt;/code&gt; command.&lt;/p&gt; &lt;pre&gt;$ &lt;b&gt;sudo MODULE=openshift OPENSHIFT_LOCAL=yes mtf-env-set&lt;/b&gt; Setting environment for module: openshift Preparing environment ... Loaded config for name: memcached Starting OpenShift Starting OpenShift using openshift/origin:v3.6.0 ... OpenShift server started. The server is accessible via web console at: https://127.0.0.1:8443 You are logged in as: User: developer Password: &amp;#60;any value&amp;#62; To login as administrator: oc login -u system:admin &lt;/pre&gt; &lt;p&gt;The &lt;code&gt;mtf-env-set&lt;/code&gt; command checks for a shell variable called &lt;code&gt;OPENSHIFT_LOCAL&lt;/code&gt;. If it is specified, the command checks if the &lt;code&gt;origin&lt;/code&gt; and &lt;code&gt;origin-clients&lt;/code&gt; packages are installed. If they are not, then it installs them.&lt;/p&gt; &lt;p&gt;In this case, a local machine performs the container testing. If you test containers on a remote OpenShift instance, you can ignore this step. If the &lt;code&gt;OPENSHIFT_LOCAL&lt;/code&gt; variable is missing, tests are executed on the remote OpenShift instance specified by the &lt;code&gt;OPENSHIFT_IP&lt;/code&gt; parameter (see below).&lt;/p&gt; &lt;h1&gt;Container Testing&lt;/h1&gt; &lt;p&gt;Now you can test your container either on a local or remote OpenShift instance by using &lt;code&gt;mtf&lt;/code&gt; command. The only difference between the following commands and the previous command is the command parameters.&lt;/p&gt; &lt;p&gt;In the following local testing case, &lt;code&gt;sanity1.py&lt;/code&gt; uses 127.0.0.1 as the value for &lt;code&gt;self.ip_address&lt;/code&gt;:&lt;/p&gt; &lt;pre&gt;$ &lt;strong&gt;sudo MODULE=openshift OPENSHIFT_USER=developer OPENSHIFT_PASSWORD=developer mtf memcached_sanity.py&lt;/strong&gt;&lt;/pre&gt; &lt;p&gt;In the following remote testing case, &lt;code&gt;sanity1.py&lt;/code&gt; uses &lt;code&gt;OPENSHIFT_IP&lt;/code&gt; as the value for &lt;code&gt;self.ip_address&lt;/code&gt;:&lt;/p&gt; &lt;pre&gt;$ &lt;strong&gt;sudo OPENSHIFT_IP=&amp;#60;ip_address&amp;#62; OPENSHIFT_USER=&amp;#60;username&amp;#62; OPENSHIFT_PASSWD=&amp;#60;passwd&amp;#62; mtf memcached_sanity.py&lt;/strong&gt;&lt;/pre&gt; &lt;p&gt;Tests are then executed from the environment where you store the configuration file and tests for the given OpenShift instance. The output looks like this:&lt;/p&gt; &lt;pre&gt;JOB ID : c2b0877ca52a14c6c740582c76f60d4f19eb2d4d JOB LOG : &lt;b&gt;/root/avocado/job-results/job-2017-12-18T12.32-c2b0877/job.log&lt;/b&gt; (1/1) memcached_sanity.py:SanityCheck1.test_smoke: &lt;b&gt;PASS&lt;/b&gt; (13.19 s) RESULTS : PASS 1 | ERROR 0 | FAIL 0 | SKIP 0 | WARN 0 | INTERRUPT 0 | CANCEL 0 JOB TIME : 13.74 s JOB HTML : /root/avocado/job-results/job-2017-12-18T12.32-c2b0877/results.html $ &lt;/pre&gt; &lt;p&gt;If you open the &lt;code&gt;&lt;b&gt;/&lt;/b&gt;root/avocado/job-results/job-2017-12-18T12.32-c2b0877/job.log&lt;/code&gt;&lt;b&gt; &lt;/b&gt;file, you’ll see contents similar to the example below.&lt;/p&gt; &lt;pre&gt;[...snip...] ['/var/log/messages', '/var/log/syslog', '/var/log/system.log']) 2017-12-18 14:29:36,208 job L0321 INFO | Command line: /bin/avocado run --json /tmp/tmppfZpNe sanity1.py 2017-12-18 14:29:36,208 job L0322 INFO | 2017-12-18 14:29:36,208 job L0326 INFO | Avocado version: 55.0 2017-12-18 14:29:36,208 job L0342 INFO | 2017-12-18 14:29:36,208 job L0346 INFO | Config files read (in order): 2017-12-18 14:29:36,208 job L0348 INFO | /etc/avocado/avocado.conf 2017-12-18 14:29:36,208 job L0348 INFO | /etc/avocado/conf.d/gdb.conf 2017-12-18 14:29:36,208 job L0348 INFO | /root/.config/avocado/avocado.conf 2017-12-18 14:29:36,208 job L0353 INFO | 2017-12-18 14:29:36,208 job L0355 INFO | Avocado config: 2017-12-18 14:29:36,209 job L0364 INFO | Section.Key [...snip...] :::::::::::::::::::::::: SETUP :::::::::::::::::::::::: 2017-12-18 14:29:36,629 avocado_test L0069 DEBUG| :::::::::::::::::::::::: START MODULE :::::::::::::::::::::::: &lt;/pre&gt; &lt;p&gt;MTF verifies whether the application exists in the OpenShift environment:.&lt;/p&gt; &lt;pre&gt;2017-12-18 14:29:36,629 process L0389 INFO | Running '&lt;b&gt;oc get dc memcached -o json&lt;/b&gt;' 2017-12-18 14:29:36,842 process L0479 DEBUG| [stderr] Error from server (NotFound): deploymentconfigs.apps.openshift.io "memcached" not found 2017-12-18 14:29:36,846 process L0499 INFO | Command 'oc get dc memcached -o json' finished with 1 after 0.213222980499s &lt;/pre&gt; &lt;p&gt;In the next step, MTF verifies whether the pod exists in OpenShift:&lt;/p&gt; &lt;pre&gt;2017-12-18 14:29:36,847 process L0389 INFO | Running '&lt;b&gt;oc get pods -o json&lt;/b&gt;' 2017-12-18 14:29:37,058 process L0479 DEBUG| [stdout] { 2017-12-18 14:29:37,059 process L0479 DEBUG| [stdout] "apiVersion": "v1", 2017-12-18 14:29:37,059 process L0479 DEBUG| [stdout] "items": [], 2017-12-18 14:29:37,059 process L0479 DEBUG| [stdout] "kind": "List", 2017-12-18 14:29:37,059 process L0479 DEBUG| [stdout] "metadata": {}, 2017-12-18 14:29:37,059 process L0479 DEBUG| [stdout] "resourceVersion": "", 2017-12-18 14:29:37,059 process L0479 DEBUG| [stdout] "selfLink": "" 2017-12-18 14:29:37,060 process L0479 DEBUG| [stdout] } 2017-12-18 14:29:37,064 process L0499 INFO | Command 'oc get pods -o json' finished with 0 after 0.211796045303s &lt;/pre&gt; &lt;p&gt;The next step creates an application with the given label &lt;code&gt;mtf_testing&lt;/code&gt; and with the name taken from the &lt;code&gt;config.yaml&lt;/code&gt; file in the &lt;code&gt;container&lt;/code&gt; tag.&lt;/p&gt; &lt;pre&gt;2017-12-18 14:29:37,064 process L0389 INFO | Running '&lt;b&gt;oc new-app -l mtf_testing=true docker.io/modularitycontainers/memcached --name=memcached&lt;/b&gt;' 2017-12-18 14:29:39,022 process L0479 DEBUG| [stdout] --&amp;#62; Found Docker image bbc8bba (5 weeks old) from docker.io for "docker.io/modularitycontainers/memcached" 2017-12-18 14:29:39,022 process L0479 DEBUG| [stdout] 2017-12-18 14:29:39,022 process L0479 DEBUG| [stdout] memcached is a high-performance, distributed memory object caching system, generic in nature, but intended for use in speeding up dynamic web applications by alleviating database load. 2017-12-18 14:29:39,022 process L0479 DEBUG| [stdout] 2017-12-18 14:29:39,022 process L0479 DEBUG| [stdout] Tags: memcached 2017-12-18 14:29:39,023 process L0479 DEBUG| [stdout] 2017-12-18 14:29:39,023 process L0479 DEBUG| [stdout] * An image stream will be created as "memcached:latest" that will track this image 2017-12-18 14:29:39,023 process L0479 DEBUG| [stdout] * This image will be deployed in deployment config "memcached" 2017-12-18 14:29:39,023 process L0479 DEBUG| [stdout] * Port 11211/tcp will be load balanced by service "memcached" 2017-12-18 14:29:39,023 process L0479 DEBUG| [stdout] * Other containers can access this service through the hostname "memcached" 2017-12-18 14:29:39,023 process L0479 DEBUG| [stdout] 2017-12-18 14:29:39,023 process L0479 DEBUG| [stdout] --&amp;#62; Creating resources with label mtf_testing=true ... 2017-12-18 14:29:39,032 process L0479 DEBUG| [stdout] imagestream "memcached" created 2017-12-18 14:29:39,043 process L0479 DEBUG| [stdout] deploymentconfig "memcached" created 2017-12-18 14:29:39,063 process L0479 DEBUG| [stdout] service "memcached" created 2017-12-18 14:29:39,064 process L0479 DEBUG| [stdout] --&amp;#62; Success 2017-12-18 14:29:39,064 process L0479 DEBUG| [stdout] Run 'oc status' to view your app. 2017-12-18 14:29:39,069 process L0499 INFO | Command 'oc new-app -l mtf_testing=true docker.io/modularitycontainers/memcached --name=memcached' finished with 0 after 2.00025391579s &lt;/pre&gt; &lt;p&gt;The next step verifies whether the application is really running and on which IP address it’s reachable:&lt;/p&gt; &lt;pre&gt;2017-12-18 14:29:46,201 process L0389 INFO | Running '&lt;b&gt;oc get service -o json&lt;/b&gt;' 2017-12-18 14:29:46,416 process L0479 DEBUG| [stdout] { 2017-12-18 14:29:46,417 process L0479 DEBUG| [stdout] "apiVersion": "v1", 2017-12-18 14:29:46,417 process L0479 DEBUG| [stdout] "items": [ 2017-12-18 14:29:46,417 process L0479 DEBUG| [stdout] { 2017-12-18 14:29:46,417 process L0479 DEBUG| [stdout] "apiVersion": "v1", 2017-12-18 14:29:46,417 process L0479 DEBUG| [stdout] "kind": "Service", 2017-12-18 14:29:46,417 process L0479 DEBUG| [stdout] "metadata": { 2017-12-18 14:29:46,417 process L0479 DEBUG| [stdout] "annotations": { 2017-12-18 14:29:46,417 process L0479 DEBUG| [stdout] "openshift.io/generated-by": "OpenShiftNewApp" 2017-12-18 14:29:46,417 process L0479 DEBUG| [stdout] }, 2017-12-18 14:29:46,418 process L0479 DEBUG| [stdout] "creationTimestamp": "2017-12-18T13:29:39Z", 2017-12-18 14:29:46,418 process L0479 DEBUG| [stdout] "labels": { 2017-12-18 14:29:46,418 process L0479 DEBUG| [stdout] "app": "memcached", 2017-12-18 14:29:46,418 process L0479 DEBUG| [stdout] "mtf_testing": "true" 2017-12-18 14:29:46,418 process L0479 DEBUG| [stdout] }, 2017-12-18 14:29:46,418 process L0479 DEBUG| [stdout] "name": "memcached", 2017-12-18 14:29:46,418 process L0479 DEBUG| [stdout] "namespace": "myproject", 2017-12-18 14:29:46,418 process L0479 DEBUG| [stdout] "resourceVersion": "2121", 2017-12-18 14:29:46,419 process L0479 DEBUG| [stdout] "selfLink": "/api/v1/namespaces/myproject/services/memcached", 2017-12-18 14:29:46,419 process L0479 DEBUG| [stdout] "uid": "7f50823d-e3f7-11e7-be28-507b9d4150cb" 2017-12-18 14:29:46,419 process L0479 DEBUG| [stdout] }, 2017-12-18 14:29:46,419 process L0479 DEBUG| [stdout] "spec": { 2017-12-18 14:29:46,419 process L0479 DEBUG| [stdout] &lt;b&gt;"clusterIP": "172.30.255.42"&lt;/b&gt;, 2017-12-18 14:29:46,419 process L0479 DEBUG| [stdout] "ports": [ 2017-12-18 14:29:46,419 process L0479 DEBUG| [stdout] { 2017-12-18 14:29:46,419 process L0479 DEBUG| [stdout] "name": "11211-tcp", 2017-12-18 14:29:46,419 process L0479 DEBUG| [stdout] "port": 11211, 2017-12-18 14:29:46,420 process L0479 DEBUG| [stdout] "protocol": "TCP", 2017-12-18 14:29:46,420 process L0479 DEBUG| [stdout] "targetPort": 11211 2017-12-18 14:29:46,420 process L0499 INFO | Command 'oc get service -o json' finished with 0 after 0.213701963425s 2017-12-18 14:29:46,420 process L0479 DEBUG| [stdout] } 2017-12-18 14:29:46,420 process L0479 DEBUG| [stdout] ], 2017-12-18 14:29:46,420 process L0479 DEBUG| [stdout] "selector": { 2017-12-18 14:29:46,420 process L0479 DEBUG| [stdout] "app": "memcached", 2017-12-18 14:29:46,421 process L0479 DEBUG| [stdout] "deploymentconfig": "memcached", 2017-12-18 14:29:46,421 process L0479 DEBUG| [stdout] "mtf_testing": "true" 2017-12-18 14:29:46,421 process L0479 DEBUG| [stdout] }, 2017-12-18 14:29:46,421 process L0479 DEBUG| [stdout] "sessionAffinity": "None", 2017-12-18 14:29:46,421 process L0479 DEBUG| [stdout] "type": "ClusterIP" 2017-12-18 14:29:46,421 process L0479 DEBUG| [stdout] }, 2017-12-18 14:29:46,421 process L0479 DEBUG| [stdout] "status": { 2017-12-18 14:29:46,421 process L0479 DEBUG| [stdout] "loadBalancer": {} 2017-12-18 14:29:46,421 process L0479 DEBUG| [stdout] } 2017-12-18 14:29:46,422 process L0479 DEBUG| [stdout] } 2017-12-18 14:29:46,422 process L0479 DEBUG| [stdout] ], 2017-12-18 14:29:46,422 process L0479 DEBUG| [stdout] "kind": "List", 2017-12-18 14:29:46,422 process L0479 DEBUG| [stdout] "metadata": {}, 2017-12-18 14:29:46,422 process L0479 DEBUG| [stdout] "resourceVersion": "", 2017-12-18 14:29:46,422 process L0479 DEBUG| [stdout] "selfLink": "" 2017-12-18 14:29:46,422 process L0479 DEBUG| [stdout] } &lt;/pre&gt; &lt;p&gt;In the last phase, tests are executed.&lt;/p&gt; &lt;pre&gt;2017-12-18 14:29:46,530 output &lt;b&gt;L0655 DEBUG| Expecting STORED&lt;/b&gt; &lt;b&gt;2017-12-18 14:29:46,531 output L0655 DEBUG| STORED was catched&lt;/b&gt; 2017-12-18 14:29:46,632 avocado_test L0069 DEBUG| :::::::::::::::::::::::: TEARDOWN :::::::::::::::::::::::: 2017-12-18 14:29:46,632 process L0389 INFO | Running 'oc get dc memcached -o json' 2017-12-18 14:29:46,841 process L0479 DEBUG| [stdout] { 2017-12-18 14:29:46,841 process L0479 DEBUG| [stdout] "apiVersion": "v1", 2017-12-18 14:29:46,841 process L0479 DEBUG| [stdout] "kind": "DeploymentConfig", 2017-12-18 14:29:46,841 process L0479 DEBUG| [stdout] "metadata": { &lt;/pre&gt; &lt;p&gt;At the end of the tests, you can verify whether the service is running in the OpenShift environment by using the command &lt;code&gt;oc status&lt;/code&gt;:&lt;/p&gt; &lt;pre&gt;$ &lt;b&gt;sudo oc status&lt;/b&gt; In project My Project (myproject) on server https://127.0.0.1:8443 You have no services, deployment configs, or build configs. Run 'oc new-app' to create an application. &lt;/pre&gt; &lt;p&gt;From this output, you can see that you can test an arbitrary container and afterward, the OpenShift environment is cleared.&lt;/p&gt; &lt;h1&gt;&lt;b&gt;Summary&lt;/b&gt;&lt;/h1&gt; &lt;p&gt;As you have seen in this article, writing tests for containers is really easy. Testing helps you guarantee that a container is working properly just as an RPM package would. In the near future, there are plans to extend MTF capabilities with&lt;a href="https://github.com/openshift/source-to-image"&gt; S2I&lt;/a&gt; testing and testing containers with OpenShift templates. You can read more in the &lt;a href="http://meta-test-family.readthedocs.io/en/latest/"&gt;MTF documentation&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;&lt;a class="a2a_button_facebook" href="https://www.addtoany.com/add_to/facebook?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2018%2F05%2F22%2Fcontainer-testing-in-openshift-with-meta-test-family%2F&amp;#38;linkname=Container%20Testing%20in%20OpenShift%20with%20Meta%20Test%20Family" title="Facebook" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_twitter" href="https://www.addtoany.com/add_to/twitter?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2018%2F05%2F22%2Fcontainer-testing-in-openshift-with-meta-test-family%2F&amp;#38;linkname=Container%20Testing%20in%20OpenShift%20with%20Meta%20Test%20Family" title="Twitter" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_google_plus" href="https://www.addtoany.com/add_to/google_plus?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2018%2F05%2F22%2Fcontainer-testing-in-openshift-with-meta-test-family%2F&amp;#38;linkname=Container%20Testing%20in%20OpenShift%20with%20Meta%20Test%20Family" title="Google+" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_linkedin" href="https://www.addtoany.com/add_to/linkedin?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2018%2F05%2F22%2Fcontainer-testing-in-openshift-with-meta-test-family%2F&amp;#38;linkname=Container%20Testing%20in%20OpenShift%20with%20Meta%20Test%20Family" title="LinkedIn" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_email" href="https://www.addtoany.com/add_to/email?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2018%2F05%2F22%2Fcontainer-testing-in-openshift-with-meta-test-family%2F&amp;#38;linkname=Container%20Testing%20in%20OpenShift%20with%20Meta%20Test%20Family" title="Email" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_print" href="https://www.addtoany.com/add_to/print?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2018%2F05%2F22%2Fcontainer-testing-in-openshift-with-meta-test-family%2F&amp;#38;linkname=Container%20Testing%20in%20OpenShift%20with%20Meta%20Test%20Family" title="Print" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_reddit" href="https://www.addtoany.com/add_to/reddit?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2018%2F05%2F22%2Fcontainer-testing-in-openshift-with-meta-test-family%2F&amp;#38;linkname=Container%20Testing%20in%20OpenShift%20with%20Meta%20Test%20Family" title="Reddit" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_flipboard" href="https://www.addtoany.com/add_to/flipboard?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2018%2F05%2F22%2Fcontainer-testing-in-openshift-with-meta-test-family%2F&amp;#38;linkname=Container%20Testing%20in%20OpenShift%20with%20Meta%20Test%20Family" title="Flipboard" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_dd addtoany_share_save addtoany_share" href="https://www.addtoany.com/share#url=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2018%2F05%2F22%2Fcontainer-testing-in-openshift-with-meta-test-family%2F&amp;#38;title=Container%20Testing%20in%20OpenShift%20with%20Meta%20Test%20Family" data-a2a-url="https://developers.redhat.com/blog/2018/05/22/container-testing-in-openshift-with-meta-test-family/" data-a2a-title="Container Testing in OpenShift with Meta Test Family"&gt;&lt;img src="https://static.addtoany.com/buttons/favicon.png" alt="Share"&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;The post &lt;a rel="nofollow" href="https://developers.redhat.com/blog/2018/05/22/container-testing-in-openshift-with-meta-test-family/"&gt;Container Testing in OpenShift with Meta Test Family&lt;/a&gt; appeared first on &lt;a rel="nofollow" href="https://developers.redhat.com/blog"&gt;RHD Blog&lt;/a&gt;.&lt;/p&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/jERJEcGQw38" height="1" width="1" alt=""/&gt;</content><summary>Without proper testing, we should not ship any container. We should guarantee that a given service in a container works properly. Meta Test Family (MTF) was designed for this very purpose. Containers can be tested as “standalone” containers and as “orchestrated” containers. Let’s look at how to test containers with the Red Hat OpenShift environment. This article describes how to do that and what a...</summary><dc:creator>unknown</dc:creator><dc:date>2018-05-22T11:00:54Z</dc:date><feedburner:origLink>https://developers.redhat.com/blog/2018/05/22/container-testing-in-openshift-with-meta-test-family/</feedburner:origLink></entry><entry><title>Narayana JDBC integration for Tomcat</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/cov_oZNFoDg/narayana-jdbc-integration-for-tomcat.html" /><category term="feed_group_name_jbosstransactions" scheme="searchisko:content:tags" /><category term="feed_name_transactions" scheme="searchisko:content:tags" /><author><name>Ondřej Chaloupka</name></author><id>searchisko:content:id:jbossorg_blog-narayana_jdbc_integration_for_tomcat</id><updated>2018-05-22T06:24:33Z</updated><published>2018-05-21T22:16:00Z</published><content type="html">&lt;p&gt; Narayana implements JTA specification in Java. It's flexible and easy to be integrated to any system which desires transaction capabilities. As proof of the Narayana extensibility check our quickstarts like &lt;a href="https://github.com/jbosstm/quickstart/tree/master/spring/narayana-spring-boot"&gt;Spring Boot one&lt;/a&gt; or &lt;a href="https://github.com/jbosstm/quickstart/tree/master/spring/camel-with-narayana-spring-boot"&gt;Camel one&lt;/a&gt;. &lt;br/&gt; But this blogpost is different integration effort. It talks in details about Narayana integration with Apache Tomcat server. &lt;/p&gt; &lt;p&gt; If you do not care about details then just jump directly to the Narayana quickstarts in this area and use the code there for yourself. &lt;ul&gt; &lt;li&gt;&lt;a href="https://github.com/jbosstm/quickstart/tree/5.8.1.Final/dbcp2-and-tomcat"&gt;dbcp2-and-tomcat&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://github.com/jbosstm/quickstart/tree/5.8.1.Final/transactionaldriver/transactionaldriver-and-tomcat"&gt;transactionaldriver-and-tomcat&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://github.com/jbosstm/quickstart/tree/5.8.1.Final/jca-and-tomcat"&gt;jca-and-tomcat&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;br/&gt; If you want more detailed understanding read further.&lt;br/&gt; All the discussed abilities are considered as the state of Narayana 5.8.1.Final or later. &lt;/p&gt; &lt;h2&gt;Narayana, database resources and JDBC interface&lt;/h2&gt; &lt;p&gt; All the proclaimed Narayana capabilities to integrate with other systems come from requirements for the system to conform with the &lt;a href="https://github.com/javaee/jta-spec"&gt;JTA specification&lt;/a&gt;. JTA expects manageable resources which follows &lt;a href="pubs.opengroup.org/onlinepubs/009680699/toc.pdf"&gt;XA specification&lt;/a&gt; in particular. For case of the database resources the underlaying API is defined by &lt;a href="https://docs.oracle.com/javase/8/docs/technotes/guides/jdbc/"&gt;JDBC specification&lt;/a&gt;. JDBC assembled resources manageable by transaction manager under package &lt;code&gt;javax.sql&lt;/code&gt; &lt;a href="https://docs.oracle.com/javase/8/docs/api/javax/sql/package-summary.html"&gt;It defines interfaces&lt;/a&gt; used for managing XA capabilities. The probably most noticable is &lt;a href="https://docs.oracle.com/javase/8/docs/api/javax/sql/XADataSource.html"&gt;XADataSource&lt;/a&gt; which serves as factory for &lt;a href="https://docs.oracle.com/javase/8/docs/api/javax/sql/XAConnection.html"&gt;XAConnection&lt;/a&gt;. From there we can otain &lt;a href="https://docs.oracle.com/javase/8/docs/api/javax/transaction/xa/XAResource.html"&gt;XAResource&lt;/a&gt;. The &lt;code&gt;XAResource&lt;/code&gt; is interface that the transaction manager works with. The instance of it participates in &lt;a href="https://developer.jboss.org/wiki/TwoPhaseCommit2PC"&gt;the two phase commit&lt;/a&gt;. &lt;/p&gt;&lt;p&gt; The workflow is to get or create the &lt;code&gt;XADataSource&lt;/code&gt;, obtains &lt;code&gt;XAConnection&lt;/code&gt; and as next the &lt;code&gt;XAResource&lt;/code&gt; which is enlisted to the global transaction (managed by a transaction manager). Now we can call queries or statements through the &lt;code&gt;XAConnection&lt;/code&gt;. When all the business work is finished the global transaction is commanded to commit which is propagated to call the commit on each enlisted &lt;code&gt;XAResource&lt;/code&gt;s. &lt;/p&gt; &lt;p&gt; It's important to mention that developer is not expected to do all this (getting xa resources, enlisting them to transaction manager&amp;hellip;) All this handling is responsibility of the "container" which could be &lt;a href="http://wildfly.org"&gt;WildFly&lt;/a&gt;, &lt;a href="https://spring.io/guides/gs/managing-transactions"&gt;Spring&lt;/a&gt; or &lt;a href="http://tomcat.apache.org/"&gt;Apache Tomcat&lt;/a&gt; in our case. &lt;br/&gt; Normally the integration which ensures the database &lt;code&gt;XAResource&lt;/code&gt; is enlisted to the transaction is provided by some &lt;i&gt;pooling library&lt;/i&gt;. By the term &lt;i&gt;pooling library&lt;/i&gt; we means code that manages a connection pool with capability enlisting database resource to the transaction. &lt;/p&gt; &lt;p&gt; We can say at the high level that integration parts are &lt;ul&gt; &lt;li&gt;the Apache Tomcat container&lt;/li&gt; &lt;li&gt;Narayana library&lt;/li&gt; &lt;li&gt;jdbc pooling library&lt;/li&gt; &lt;/ul&gt; &lt;br/&gt; In this article we will talk about &lt;a href="https://github.com/jbosstm/narayana/tree/5.8.1.Final/ArjunaJTA/jdbc"&gt;Narayana JDBC transactional driver&lt;/a&gt;, &lt;a href="https://github.com/apache/commons-dbcp"&gt;Apache Commons DBCP&lt;/a&gt; and &lt;a href="https://github.com/ironjacamar/ironjacamar"&gt;IronJacamar&lt;/a&gt;. &lt;/p&gt; &lt;h2&gt;Narayana configuration with Tomcat&lt;/h2&gt; &lt;p&gt; After the brief overview of integration requirements, let's elaborate on common settings needed for any integration approach you choose.&lt;br/&gt; Be aware that each library needs a little bit different configuration and especially IronJacamar is specific. &lt;/p&gt; &lt;h3&gt;JDBC pooling libraries integration&lt;/h3&gt; &lt;p&gt; Narayana provides integration code in maven module &lt;a href="https://github.com/jbosstm/narayana/tree/5.8.1.Final/tomcat/tomcat-jta"&gt;&lt;code&gt;tomcat-jta&lt;/code&gt;&lt;/a&gt;. That contains the glue code which integrates Narayana to the world of the Tomcat. If you write an application you will need the following: &lt;ul&gt; &lt;li&gt;providing Narayana itself to the application classpath&lt;/li&gt; &lt;li&gt;providing Narayana &lt;code&gt;tomcat-jta&lt;/code&gt; module to the application classpath&lt;/li&gt; &lt;li&gt;configure &lt;code&gt;WEB-INF/web.xml&lt;/code&gt; with &lt;code&gt;NarayanaJtaServletContextListener&lt;/code&gt; which ensures the intialization of Narayana transaction manager&lt;/li&gt; &lt;li&gt;add &lt;code&gt;META-INF/context.xml&lt;/code&gt; which setup Tomcat to start using implementation of JTA interfaces provided by Narayana&lt;/li&gt; &lt;li&gt;configure database resources to be XA aware and cooperate with Narayana by setting them up in the &lt;code&gt;META-INF/context.xml&lt;/code&gt;&lt;/li&gt; &lt;/ul&gt; &lt;br/&gt; &lt;i&gt;NOTE:&lt;/i&gt; if you expect to use the IronJacamar this requirements differs a bit! &lt;/p&gt; &lt;p&gt; If we take a look at the structure of the jar to be deployed we would get the picture possibly similar to this one: &lt;pre&gt;&lt;br /&gt; ├── META-INF&lt;br /&gt; │   └── context.xml&lt;br /&gt; └── WEB-INF&lt;br /&gt; ├── classes&lt;br /&gt; │   ├── application&amp;hellip;&lt;br /&gt; │   └── jbossts-properties.xml&lt;br /&gt; ├── lib&lt;br /&gt; │   ├── arjuna-5.8.1.Final.jar&lt;br /&gt; │   ├── jboss-logging-3.2.1.Final.jar&lt;br /&gt; │   ├── jboss-transaction-spi-7.6.0.Final.jar&lt;br /&gt; │   ├── jta-5.8.1.Final.jar&lt;br /&gt; │   ├── postgresql-9.0-801.jdbc4.jar&lt;br /&gt; │   └── tomcat-jta-5.8.1.Final.jar&lt;br /&gt; └── web.xml&lt;br /&gt; &lt;/pre&gt;&lt;/p&gt; &lt;p&gt; From this summary let's overview the configuration files one by one to see what's needed to be defined there. &lt;/p&gt; &lt;h3&gt;Configuration files to be setup for the integration&lt;/h3&gt; &lt;h4&gt;WEB-INF/web.xml&lt;/h4&gt;&lt;p&gt; &lt;pre&gt;&lt;code class="xml"&gt;&lt;br /&gt; &amp;lt;web-app&amp;gt;&lt;br /&gt; &amp;lt;listener&amp;gt;&lt;br /&gt; &amp;lt;listener-class&amp;gt;org.jboss.narayana.tomcat.jta.NarayanaJtaServletContextListener&amp;lt;/listener-class&amp;gt;&lt;br /&gt; &amp;lt;/listener&amp;gt;&lt;br /&gt; &amp;lt;/web-app&amp;gt;&lt;br /&gt; &lt;/code&gt;&lt;/pre&gt;&lt;/p&gt;&lt;p&gt; The &lt;code&gt;web.xml&lt;/code&gt; needs to define the &lt;a href="https://github.com/jbosstm/narayana/blob/5.8.1.Final/tomcat/tomcat-jta/src/main/java/org/jboss/narayana/tomcat/jta/NarayanaJtaServletContextListener.java"&gt;NarayanaJtaServletContextListener&lt;/a&gt; to be loaded during context initialization to initialize the Narayana itself. Narayana needs to get running, for example, reaper thread that ensures transaction timeouts checking or thread of recovery manager. &lt;/p&gt; &lt;h4&gt;WEB-INF/clases/jbossts-properties.xml&lt;/h4&gt;&lt;p&gt; This file is not compulsory. The purpose is to configure the Narayana itself.&lt;br/&gt; If you don't use your own configuration file then the default is in charge. See more at blogpost &lt;a href="https://jbossts.blogspot.cz/2018/01/narayana-periodic-recovery-of-xa.html#configuration"&gt; Narayana periodic recovery of XA transactions &lt;/a&gt; or consider settings done by the default descriptor &lt;a href="https://github.com/jbosstm/narayana/blob/5.8.1.Final/ArjunaJTS/narayana-jts-idlj/src/main/resources/jbossts-properties.xml"&gt;jbossts-properties.xml at narayana-jts-idlj&lt;/a&gt;. &lt;/p&gt; &lt;h4&gt;META-INF/context.xml&lt;/h4&gt; &lt;p&gt; &lt;pre&gt;&lt;code class="xml"&gt;&lt;br /&gt; &amp;lt;?xml version="1.0" encoding="UTF-8" standalone="no"?&amp;gt;&lt;br /&gt; &amp;lt;Context antiJarLocking="true" antiResourceLocking="true"&amp;gt;&lt;br /&gt; &amp;lt;!-- Narayana resources --&amp;gt;&lt;br /&gt; &amp;lt;Transaction factory="org.jboss.narayana.tomcat.jta.UserTransactionFactory"/&amp;gt;&lt;br /&gt; &amp;lt;Resource factory="org.jboss.narayana.tomcat.jta.TransactionManagerFactory"&lt;br /&gt; name="TransactionManager" type="javax.transaction.TransactionManager"/&amp;gt;&lt;br /&gt; &amp;lt;Resource factory="org.jboss.narayana.tomcat.jta.TransactionSynchronizationRegistryFactory"&lt;br /&gt; name="TransactionSynchronizationRegistry" type="javax.transaction.TransactionSynchronizationRegistry"/&amp;gt;&lt;br /&gt;&lt;br /&gt; &amp;lt;Resource auth="Container" databaseName="test" description="Data Source"&lt;br /&gt; factory="org.postgresql.xa.PGXADataSourceFactory" loginTimeout="0"&lt;br /&gt; name="myDataSource" password="test" portNumber="5432" serverName="localhost"&lt;br /&gt; type="org.postgresql.xa.PGXADataSource" user="test" username="test"&lt;br /&gt; uniqueName="myDataSource" url="jdbc:postgresql://localhost:5432/test"/&amp;gt;&lt;br /&gt; &amp;lt;Resource auth="Container" description="Transactional Data Source"&lt;br /&gt; factory="org.jboss.narayana.tomcat.jta.TransactionalDataSourceFactory"&lt;br /&gt; initialSize="10" jmxEnabled="true" logAbandoned="true" maxAge="30000"&lt;br /&gt; maxIdle="16" maxTotal="4" maxWaitMillis="10000" minIdle="8"&lt;br /&gt; name="transactionalDataSource" password="test" removeAbandoned="true"&lt;br /&gt; removeAbandonedTimeout="60" testOnBorrow="true" transactionManager="TransactionManager"&lt;br /&gt; type="javax.sql.XADataSource" uniqueName="transactionalDataSource"&lt;br /&gt; username="test" validationQuery="select 1" xaDataSource="myDataSource"/&amp;gt;&lt;br /&gt; &amp;lt;/Context&amp;gt;&lt;br /&gt; &lt;/code&gt;&lt;/pre&gt;&lt;/p&gt; &lt;p&gt; I divide explanation this file into two parts. First are the generic settings - those needed for transaction manager integration (top part of the &lt;code&gt;context.xml&lt;/code&gt;). The second part is on resource declaration that defines linking to the JDBC pooling library. &lt;/p&gt; &lt;h5&gt;Transaction manager integration settings&lt;/h5&gt; &lt;p&gt; We define implementation classes for the &lt;a href="https://github.com/eclipse-ee4j/jta-api"&gt;JTA api&lt;/a&gt; here. The implementation is provided by Narayana transaction manager. Those are lines of &lt;code&gt;UserTransactionFactory&lt;/code&gt; and resources of &lt;code&gt;TransactionManager&lt;/code&gt; and &lt;code&gt;TransactionSynchronizationRegistry&lt;/code&gt; in the &lt;code&gt;context.xml&lt;/code&gt; file.&lt;br/&gt;&lt;/p&gt; &lt;h5&gt;JDBC pooling library settings&lt;/h5&gt; &lt;p&gt; We aim to define database resources that can be used in the application. That's how you &lt;a href="https://github.com/jbosstm/quickstart/blob/master/dbcp2-and-tomcat/src/main/java/io/narayana/StringDao.java#L119"&gt;get the connection&lt;/a&gt; typically with code &lt;code class="java"&gt;DataSource ds = InitialContext.doLookup("java:comp/env/transactionalDataSource")&lt;/code&gt;, and eventually &lt;a href="https://github.com/jbosstm/quickstart/blob/master/dbcp2-and-tomcat/src/main/java/io/narayana/StringDao.java#L81"&gt;execute a sql statement&lt;/a&gt;. &lt;br/&gt; We define a PostgreSQL datasource with information how to create a new XA connection (we provide the host and port, credentials etc.) in the example.&lt;br/&gt; The second resource is definition of jdbc pooling library to utilize the PostgreSQL one and to provide the XA capabilities. It roughtly means putting the PostgreSQL connection to the managed pool and enlisting the work under an active transaction. &lt;br/&gt; Thus we have got two resources defined here. One is non-managed (the PosgreSQL one) and the second manages the first one to provide the ease work with the resources. For the developer is the most important to know he needs to use the managed one in his application, namely the &lt;code&gt;transactionalDataSource&lt;/code&gt; from our example. &lt;/p&gt; &lt;h5&gt;A bit about datasource configuration of Apache Tomcat context.xml&lt;/h5&gt; &lt;p&gt; Let's take a side step at this place. Before we will talk in details about supported pooling libraries let's check a little bit more about the configuration of the &lt;code&gt;Resource&lt;/code&gt; from perspective of XA connection in the &lt;code&gt;context.xml&lt;/code&gt;. &lt;/p&gt;&lt;p&gt; Looking at the &lt;code&gt;Resource&lt;/code&gt; definition there are highlighted parts which are interesting for us &lt;pre&gt;&lt;br /&gt; &amp;lt;Resource auth="Container" &lt;span style="color:rgb(255, 150, 200)"&gt;databaseName="test"&lt;/span&gt; description="Data Source"&lt;br /&gt; &lt;span style="color:rgb(255, 150, 200)"&gt;factory="org.postgresql.xa.PGXADataSourceFactory"&lt;/span&gt;&lt;br /&gt; loginTimeout="0" &lt;span style="color:rgb(255, 150, 200)"&gt;name="myDataSource"&lt;/span&gt; &lt;span style="color:rgb(255, 150, 200)"&gt;password="test"&lt;/span&gt; &lt;span style="color:rgb(255, 150, 200)"&gt;portNumber="5432"&lt;/span&gt; &lt;span style="color:rgb(255, 150, 200)"&gt;serverName="localhost"&lt;/span&gt;&lt;br /&gt; &lt;span style="color:rgb(255, 150, 200)"&gt;type="org.postgresql.xa.PGXADataSource"&lt;/span&gt; uniqueName="myDataSource"&lt;br /&gt; url="jdbc:postgresql://localhost:5432/test" &lt;span style="color:rgb(255, 150, 200)"&gt;user="test"&lt;/span&gt; username="test"/&amp;gt;&lt;br /&gt; &lt;/pre&gt;&lt;/p&gt;&lt;p&gt; &lt;ul&gt; &lt;dl&gt; &lt;dt&gt;&lt;code&gt;name&lt;/code&gt;&lt;/dt&gt; &lt;dd&gt;defines the name the resource is bound at the container and we can use the jndi lookup to find it by that name in application&lt;/dd&gt; &lt;dt&gt;&lt;code&gt;factory&lt;/code&gt;&lt;/dt&gt; &lt;dd&gt;defines what type we will get as the final created &lt;code&gt;Object&lt;/code&gt;. The factory which we declares here is class which implements interface &lt;a href="https://docs.oracle.com/javase/8/docs/api/javax/naming/spi/ObjectFactory.html"&gt;ObjectFactory&lt;/a&gt; and from the provided properties it construct an object.&lt;br/&gt; If we would not define any &lt;code&gt;factory&lt;/code&gt; element in the definition then the Tomcat class &lt;a href="https://github.com/apache/tomcat/blob/trunk/java/org/apache/naming/factory/ResourceFactory.java#L42"&gt;ResourceFactory&lt;/a&gt; is used (see &lt;a href="https://github.com/apache/tomcat/blob/trunk/java/org/apache/naming/factory/Constants.java#L26"&gt;default factory constants&lt;/a&gt;). The &lt;code&gt;ResourceFactory&lt;/code&gt; will pass the call to the &lt;a href="https://github.com/apache/tomcat/blob/trunk/java/org/apache/tomcat/dbcp/dbcp2/BasicDataSourceFactory.java"&gt;BasicDataSourceFactory&lt;/a&gt; of the dbcp2 library. Here we can see the importantce of the &lt;code&gt;type&lt;/code&gt; xml parameter which defines what is the object type we want to obtain and the factory normally checks if it's able to provide such (by &lt;a href="https://github.com/apache/tomcat/blob/trunk/java/org/apache/tomcat/dbcp/dbcp2/BasicDataSourceFactory.java#L251"&gt;string equals check&lt;/a&gt; usually).&lt;br/&gt; The next step is generation of the object itself where the factory takes each of the &lt;a href="https://github.com/apache/tomcat/blob/trunk/java/org/apache/tomcat/dbcp/dbcp2/BasicDataSourceFactory.java#L365"&gt;properties and tries to applied&lt;/a&gt; them. &lt;br/&gt; In our case we use the &lt;a href="https://github.com/pgjdbc/pgjdbc/blob/master/pgjdbc/src/main/java/org/postgresql/xa/PGXADataSourceFactory.java"&gt;PGXADataSourceFactory&lt;/a&gt; which utilizes &lt;a href="https://github.com/pgjdbc/pgjdbc/blob/master/pgjdbc/src/main/java/org/postgresql/ds/common/BaseDataSource.java#L1212"&gt;some of the properties to create the &lt;code&gt;XADataSource&lt;/code&gt;&lt;/a&gt;. &lt;/dd&gt; &lt;dt&gt;&lt;code&gt;serverName&lt;/code&gt;, &lt;code&gt;portNumber&lt;/code&gt;, &lt;code&gt;databaseName&lt;/code&gt;, &lt;code&gt;user&lt;/code&gt;, &lt;code&gt;password&lt;/code&gt;&lt;/dt&gt; &lt;dd&gt;are properties used by the &lt;code&gt;object factory&lt;/code&gt; class to get connection from the database &lt;br/&gt; Knowing the name of the properties for the particular &lt;code&gt;ObjectFactory&lt;/code&gt; is possibly the most important when you need to configure your datasource. Here you need to check &lt;code&gt;setters&lt;/code&gt; of the factory implementation. &lt;br/&gt; In case of the &lt;code&gt;PGXADataSourceFactory&lt;/code&gt; we need to go through the inheritance hierarchy to find the properties are saved at &lt;a href="https://github.com/pgjdbc/pgjdbc/blob/master/pgjdbc/src/main/java/org/postgresql/ds/common/BaseDataSource.java"&gt;BaseDataSource&lt;/a&gt;. For our case for the relevant properties are &lt;code&gt;user name&lt;/code&gt; and &lt;code&gt;password&lt;/code&gt;. From the &lt;code&gt;BaseDataSource&lt;/code&gt; we can see the setter for the user name is &lt;a href="https://github.com/pgjdbc/pgjdbc/blob/master/pgjdbc/src/main/java/org/postgresql/ds/common/BaseDataSource.java#L190"&gt;setUser&lt;/a&gt; thus the property name we look for is &lt;code&gt;user&lt;/code&gt;. &lt;/dd&gt; &lt;/dl&gt; &lt;/ul&gt;&lt;/p&gt; &lt;p&gt; After this side step let's take a look at the setup of the &lt;code&gt;Resource&lt;/code&gt;s in respect of the used pooling library. &lt;/p&gt; &lt;h3&gt;Apache Commons DBCP2 library&lt;/h3&gt; &lt;p&gt;&lt;b&gt;Quickstart:&lt;/b&gt; &lt;a href="https://github.com/jbosstm/quickstart/tree/master/dbcp2-and-tomcat"&gt;https://github.com/jbosstm/quickstart/tree/master/dbcp2-and-tomcat&lt;/a&gt;&lt;/p&gt;&lt;p&gt; The best integration comes probably with Apache Common DBCP2 as the library itself is part of the Tomcat distribution (the Tomcat code base uses &lt;a href="https://github.com/apache/tomcat/tree/trunk/java/org/apache/tomcat/dbcp/dbcp2"&gt;fork of the project&lt;/a&gt;). The XA integration is provided in Apache Tomcat version 9.0.7 and later. There is added dbcp2 package &lt;a href="https://github.com/apache/tomcat/tree/trunk/java/org/apache/tomcat/dbcp/dbcp2/managed"&gt;managed&lt;/a&gt; which knows how to enlist a resource to XA transaction. &lt;/p&gt; &lt;p&gt; The integration is similar to what we discussed in case of the JDBC transactional driver. You need to have configured two resources in &lt;code&gt;context.xml&lt;/code&gt;. One is the database datasource (&lt;a href="#contextxmldatasource"&gt;see above&lt;/a&gt;) and other is wrapper providing XA capabilities. &lt;/p&gt; &lt;pre&gt;&lt;code style="xml"&gt;&lt;br /&gt; &amp;lt;Resource name="transactionalDataSource" uniqueName="transactionalDataSource"&lt;br /&gt; auth="Container" type="javax.sql.XADataSource"&lt;br /&gt; transactionManager="TransactionManager" xaDataSource="h2DataSource"&lt;br /&gt; factory="org.jboss.narayana.tomcat.jta.TransactionalDataSourceFactory"/&amp;gt;&lt;br /&gt;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt; The integration is here done over the use of the specific &lt;code&gt;factory&lt;/code&gt; which directly depends on classes from Apache Tomcat &lt;code&gt;org.apache.tomcat.dbcp.dbcp2&lt;/code&gt; package. The factory ensures the resource being enlisted to the recovery manager as well.&lt;br/&gt; The nice feature is that you can use all the DBCP2 configuration parameters for pooling as you would used when &lt;code&gt;BasicDataSource&lt;/code&gt; is configured. See the configuration options and the their meaning at the &lt;a href="https://commons.apache.org/proper/commons-dbcp/configuration.html"&gt;Apache Commons documentation&lt;/a&gt;. &lt;/p&gt; &lt;p&gt; &lt;b&gt;Summary:&lt;/b&gt; &lt;ul&gt; &lt;li&gt;Already packed in the &lt;code&gt;Apache Tomcat&lt;/code&gt; distribution from version 9.0.7&lt;/li&gt; &lt;li&gt;Configure two resources in &lt;code&gt;context.xml&lt;/code&gt;. One is the database datasource, the second is wrapper providing XA capabilities with use of the &lt;code&gt;dbcp2&lt;/code&gt; pooling capabilities integrated with &lt;code&gt;TransactionalDataSourceFactory&lt;/code&gt;.&lt;/li&gt; &lt;/ul&gt;&lt;/p&gt; &lt;h3&gt;Narayana jdbc transactional driver&lt;/h3&gt; &lt;p&gt;&lt;b&gt;Quickstart:&lt;/b&gt; &lt;a href="https://github.com/jbosstm/quickstart/tree/master/transactionaldriver/transactionaldriver-and-tomcat"&gt;https://github.com/jbosstm/quickstart/tree/master/transactionaldriver/transactionaldriver-and-tomcat&lt;/a&gt;&lt;/p&gt;&lt;p&gt;With this we will get back to other two recent articles about &lt;a href="https://jbossts.blogspot.cz/2017/12/narayana-jdbc-transactional-driver.html"&gt;jdbc transactional driver&lt;/a&gt; and &lt;a href="https://jbossts.blogspot.cz/2018/01/recovery-of-narayana-jdbc-transactional.html"&gt;recovery of the transactional driver&lt;/a&gt;.&lt;br/&gt;The big advantage of jdbc transactional driver is its tight integration with Narayana. It's the dependecy of the &lt;a href="https://github.com/jbosstm/narayana/tree/master/tomcat/tomcat-jta"&gt;Narayana &lt;code&gt;tomcat-jta&lt;/code&gt;&lt;/a&gt;module which contains all the integration code needed for Narayana working in Tomcat. So if you take the &lt;code&gt;tomcat-jta-5.8.1.Final&lt;/code&gt;you have packed the Narayna integration code and jdbc driver in out-of-the-box working bundle. &lt;/p&gt; &lt;h4&gt;Configuration actions&lt;/h4&gt;&lt;p&gt; Here we will define two resources in the &lt;code&gt;context.xml&lt;/code&gt; file. The first one is &lt;a href="https://github.com/jbosstm/quickstart/blob/5.8.1.Final/transactionaldriver/transactionaldriver-and-tomcat/src/main/webapp/META-INF/context.xml#L11"&gt;the database one&lt;/a&gt;. &lt;/p&gt; &lt;a id="contextxmldatasource" name="contextxmldatasource"&gt;&lt;/a&gt;&lt;pre&gt;&lt;code style="xml"&gt;&lt;br /&gt; &amp;lt;Resource name="h2DataSource" uniqueName="h2Datasource" auth="Container"&lt;br /&gt; type="org.h2.jdbcx.JdbcDataSource" username="sa" user="sa" password="sa"&lt;br /&gt; url="jdbc:h2:mem:testdb;DB_CLOSE_DELAY=-1" description="H2 Data Source"&lt;br /&gt; loginTimeout="0" factory="org.h2.jdbcx.JdbcDataSourceFactory"/&amp;gt;&lt;br /&gt;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt; The database one defines data needed for preparation of datasource and creation of the connection. The datasource is not XA aware. We need to add one more layer on top which is transactional JDBC driver here. &lt;a href="https://github.com/jbosstm/quickstart/blob/5.8.1.Final/transactionaldriver/transactionaldriver-and-tomcat/src/main/webapp/META-INF/context.xml#L14"&gt;It wraps the datasource connection&lt;/a&gt; within XA capabilities. &lt;/p&gt; &lt;pre&gt;&lt;code style="xml"&gt;&lt;br /&gt; &amp;lt;Resource name="transactionalDataSource" uniqueName="transactionalDataSource"&lt;br /&gt; auth="Container" type="javax.sql.DataSource" username="sa" password="sa"&lt;br /&gt; driverClassName="com.arjuna.ats.jdbc.TransactionalDriver"&lt;br /&gt; url="jdbc:arjuna:java:comp/env/h2DataSource" description="Transactional Driver Datasource"&lt;br /&gt; connectionProperties="POOL_CONNECTIONS=false"/&amp;gt;&lt;br /&gt;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt; As we do not define the element &lt;code&gt;factory&lt;/code&gt; then the default one is used which is &lt;code&gt;org.apache.tomcat.dbcp.dbcp2.BasicDataSourceFactory&lt;/code&gt;. Unfortunately, this is fine up to the time you need to process some more sophisticated pooling strategies. In this aspect the transactional driver does not play well with the default factory and some further integration work would be needed. &lt;/p&gt; &lt;p&gt; This configuration is nice for having &lt;code&gt;transactionalDataSource&lt;/code&gt; available for the transactional work. Unfortunately, it's not all that you need to do. You miss here configuration of recovery. You need to tell the recovery manager what is the resource to care of. You can setup this in &lt;code&gt;jbossts-properties.xml&lt;/code&gt; or maybe easier way to add it to &lt;a href="https://github.com/jbosstm/quickstart/blob/5.8.1.Final/transactionaldriver/transactionaldriver-and-tomcat/run.sh#L10"&gt;environment variables of the starting Tomcat&lt;/a&gt;, for example by adding the setup under script &lt;code&gt;$CATALINA_HOME/bin/setenv.sh&lt;/code&gt;&lt;br/&gt; You define it with property &lt;a href="https://github.com/jbosstm/narayana/blob/master/ArjunaJTA/jta/classes/com/arjuna/ats/jta/common/JTAEnvironmentBean.java#L66"&gt;com.arjuna.ats.jta.recovery.XAResourceRecovery&lt;/a&gt;. &lt;/p&gt; &lt;pre&gt;&lt;code style="bash"&gt;&lt;br /&gt;-Dcom.arjuna.ats.jta.recovery.XAResourceRecovery1=com.arjuna.ats.internal.jdbc.recovery.BasicXARecovery;abs://$(pwd)/src/main/resources/h2recoveryproperties.xml&lt;br /&gt;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt; You can define whatever number of the resources you need the recovery is aware of. It's done by adding more &lt;code&gt;numbers&lt;/code&gt; at the end of the property name (we use &lt;code&gt;1&lt;/code&gt; in the example above). The value of the property is the class implementing &lt;code&gt;com.arjuna.ats.jta.recovery.XAResourceRecovery&lt;/code&gt;. All the properties provided to the particular implementation is concatenated after the &lt;code&gt;;&lt;/code&gt; character. In our example it's path to the &lt;code&gt;xml descriptor h2recoveryproperties.xml&lt;/code&gt;.&lt;br/&gt; When transactional driver is used then you need to declare&lt;code&gt;BasicXARecovery&lt;/code&gt; as recovery implementation class and this class needs &lt;a href="https://github.com/jbosstm/quickstart/blob/5.8.1.Final/transactionaldriver/transactionaldriver-and-tomcat/src/main/resources/h2recoveryproperties.xml"&gt;connection properties&lt;/a&gt; to be declared in the xml descriptor. &lt;/p&gt; &lt;pre&gt;&lt;code style="xml"&gt;&lt;br /&gt; &amp;lt;?xml version="1.0" encoding="UTF-8"?&amp;gt;&lt;br /&gt;&amp;lt;!DOCTYPE properties SYSTEM "http://java.sun.com/dtd/properties.dtd"&amp;gt;&lt;br /&gt;&amp;lt;properties&amp;gt;&lt;br /&gt; &amp;lt;entry key="DB_1_DatabaseUser"&amp;gt;sa&amp;lt;/entry&amp;gt;&lt;br /&gt; &amp;lt;entry key="DB_1_DatabasePassword"&amp;gt;sa&amp;lt;/entry&amp;gt;&lt;br /&gt; &amp;lt;entry key="DB_1_DatabaseDynamicClass"&amp;gt;&amp;lt;/entry&amp;gt;&lt;br /&gt; &amp;lt;entry key="DB_1_DatabaseURL"&amp;gt;java:comp/env/h2DataSource&amp;lt;/entry&amp;gt;&lt;br /&gt;&amp;lt;/properties&amp;gt;&lt;br /&gt;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt; Note: there is option not defining the two resources under &lt;code&gt;context.xml&lt;/code&gt; and use the env property for recovery enlistment. All the configuration properties are then involved in one &lt;code&gt;properties&lt;/code&gt; file and &lt;a href="https://jbossts.blogspot.cz/2017/12/narayana-jdbc-transactional-driver.html"&gt;transactional driver dynamic class&lt;/a&gt; is used. If interested the working example is at &lt;a href="https://github.com/ochaloup/quickstart-jbosstm/tree/transactional-driver-and-tomcat-dynamic-class/transactionaldriver/transactionaldriver-and-tomcat"&gt;ochaloup/quickstart-jbosstm/tree/transactional-driver-and-tomcat-dynamic-class&lt;/a&gt;. &lt;/p&gt; &lt;p&gt; &lt;b&gt;Summary:&lt;/b&gt; &lt;ul&gt; &lt;li&gt;Already packed in the &lt;code&gt;tomcat-jta&lt;/code&gt; artifact&lt;/li&gt; &lt;li&gt;Configure two resources in &lt;code&gt;context.xml&lt;/code&gt;. One is database datasource, the second is transactional datasource wrapped by transactional driver.&lt;/li&gt; &lt;li&gt;Need to configure recovery with env variable setup &lt;code&gt;com.arjuna.ats.jta.recovery.XAResourceRecovery&lt;/code&gt; while providing xml descriptor with connection parameters&lt;/li&gt; &lt;/ul&gt;&lt;/p&gt; &lt;h3&gt;IronJacamar&lt;/h3&gt;&lt;p&gt;&lt;b&gt;Quickstart:&lt;/b&gt; &lt;a href="https://github.com/jbosstm/quickstart/tree/master/jca-and-tomcat"&gt;https://github.com/jbosstm/quickstart/tree/master/jca-and-tomcat&lt;/a&gt;&lt;/p&gt; &lt;p&gt; The settings of IronJacamar integration differs pretty much from what we've seen so far. The IronJacamar implements whole JCA specification and it's pretty different beast (not &lt;i&gt;just&lt;/i&gt; a jdbc pooling library). &lt;/p&gt; &lt;p&gt; The whole handling and integration is passed to IronJacamar itself.&lt;br/&gt; You don't use &lt;code&gt;tomcat-jta&lt;/code&gt; module at all.&lt;br/&gt; You need to configure all aspects in the IronJacamar &lt;code&gt;xml descriptors&lt;/code&gt;. Aspects like datasource definition, transaction configuration, pooling definition, up to the jndi binding. &lt;/p&gt; &lt;p&gt; The standalone IronJacamar is needed to be started with command &lt;code&gt;org.jboss.jca.embedded.EmbeddedFactory.create().startup()&lt;/code&gt; where you &lt;a href="https://github.com/jbosstm/quickstart/blob/5.8.1.Final/jca-and-tomcat/src/test/java/org/jboss/narayana/quickstart/jca/common/AbstractTest.java#L51"&gt;defines the descriptors to be used&lt;/a&gt;. You can configure it in &lt;code&gt;web.xml&lt;/code&gt; as &lt;a href="https://github.com/jbosstm/quickstart/blob/5.8.1.Final/jca-and-tomcat/src/main/java/org/jboss/narayana/quickstart/jca/listener/ServletContextListenerImpl.java"&gt;&lt;code&gt;ServletContextListener&lt;/code&gt;&lt;/a&gt;. &lt;/p&gt; &lt;p&gt; What are descriptors to be defined: &lt;ul&gt; &lt;li&gt;&lt;a href="https://github.com/jbosstm/quickstart/blob/5.8.1.Final/jca-and-tomcat/src/main/resources/jdbc-xa.rar"&gt;jdbc-xa.rar&lt;/a&gt; which is resource adapter provided by IronJacamar itself. It needs to be part of the deployment. It's capable to process &lt;code&gt;ds&lt;/code&gt; files. &lt;/li&gt; &lt;li&gt;&lt;a href="https://github.com/jbosstm/quickstart/blob/5.8.1.Final/jca-and-tomcat/src/main/resources/postgres-xa-ds.xml"&gt;ds.xml&lt;/a&gt; which defines connecion properties and jndi name binding &lt;/li&gt; &lt;li&gt;&lt;a href="https://github.com/jbosstm/quickstart/blob/5.8.1.Final/jca-and-tomcat/src/main/resources/transaction.xml"&gt;transaction.xml&lt;/a&gt; which configures transaction manager instead of use of the &lt;code&gt;jbossts-properties.xml&lt;/code&gt;. &lt;/li&gt; &lt;/ul&gt; Check more configuration in &lt;a href="http://www.ironjacamar.org/doc/userguide/1.2/en-US/html_single/index.html"&gt;IronJacamar documentation&lt;/a&gt;. &lt;/p&gt; &lt;p&gt; &lt;b&gt;Summary:&lt;/b&gt; IronJacamar is started as embedded system and process all the handling on its own. Developer needs to provide xml descriptor to set up. &lt;/p&gt; &lt;h2&gt;Summary&lt;/h2&gt; &lt;p&gt; This article provides the details about configuration of the Narayana when used in Apache Tomcat container. We've seen the three possible libraries to get the integration working - the Narayana JDBC transactional driver, Apache DBCP2 library and IronJacamar JCA implementation.&lt;br/&gt; On top of it, the article contains many details about Narayana and Tomcat resource configuration. &lt;/p&gt; &lt;p&gt;If you hesitate what alternative is the best fit for your project then this table can help you &lt;table border="1" style="border-collapse: collapse; width: 100%; border: 1px solid #dddddd; text-align: left; padding: 8px;"&gt; &lt;tr style="background-color: #dddddd;"&gt; &lt;th width="180"&gt;JDBC integration library&lt;/th&gt; &lt;th&gt;When to use&lt;/th&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Apache DBCP2&lt;/td&gt; &lt;td&gt;It's the recommended option when you want to obtain Narayana transaction handling in the Apache Tomcat Integration is done in the Narayana resource factory which ensures easily setting up the datasource and recovery in the one step. &lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Narayana transactional jdbc driver&lt;/td&gt; &lt;td&gt;Is good fit when you want to have all parts integrated and covered by Narayana project. It provides lightweight JDBC pooling layer that could be nice for small projects. Integration requires a little bit more hand working. &lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;IronJacamar&lt;/td&gt; &lt;td&gt;To be used when you need whole JCA functionality running in Apache Tomcat. The benefit of this solution is the battle tested integration of Narayana and IronJacamar as they are delivered as one pack in the WildFly application server. &lt;/td&gt; &lt;/tr&gt;&lt;/table&gt;&lt;/p&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/cov_oZNFoDg" height="1" width="1" alt=""/&gt;</content><summary>Narayana implements JTA specification in Java. It's flexible and easy to be integrated to any system which desires transaction capabilities. As proof of the Narayana extensibility check our quickstarts like Spring Boot one or Camel one. But this blogpost is different integration effort. It talks in details about Narayana integration with Apache Tomcat server. If you do not care about details then ...</summary><dc:creator>Ondřej Chaloupka</dc:creator><dc:date>2018-05-21T22:16:00Z</dc:date><feedburner:origLink>http://jbossts.blogspot.com/2018/05/narayana-jdbc-integration-for-tomcat.html</feedburner:origLink></entry><entry><title>Red Hat Summit: Lowering the risk of monolith to microservices</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/fz4rA-JfAIc/" /><category term="enterprise architecture" scheme="searchisko:content:tags" /><category term="feed_group_name_nonmiddleware" scheme="searchisko:content:tags" /><category term="feed_name_redhat_developer_blog" scheme="searchisko:content:tags" /><category term="microservices" scheme="searchisko:content:tags" /><category term="Modern App Dev" scheme="searchisko:content:tags" /><category term="modernization" scheme="searchisko:content:tags" /><category term="red hat summit" scheme="searchisko:content:tags" /><category term="Red Hat Summit 2018" scheme="searchisko:content:tags" /><author><name>unknown</name></author><id>searchisko:content:id:jbossorg_blog-red_hat_summit_lowering_the_risk_of_monolith_to_microservices</id><updated>2018-05-21T17:57:33Z</updated><published>2018-05-21T17:57:33Z</published><content type="html">&lt;p&gt;Christian Posta, Chief Architect at Red Hat, presented the story of a fictitious company&lt;sup&gt;1&lt;/sup&gt; moving a monolithic application to microservices.&lt;/p&gt; &lt;p&gt;When considering risk, we think we know the bad things that can happen and the probabilities of those bad things actually happening. Christian defines a &lt;em&gt;monolith&lt;/em&gt; as a large application developed over many years by different teams that delivers proven business value while being very difficult to update and maintain. Its architecture, elegant at one point, has eroded over time. That makes it difficult to assess the risk of migrating a monolith.&lt;/p&gt; &lt;p&gt;Let&amp;#8217;s hear from the new school:&lt;/p&gt; &lt;blockquote&gt;&lt;p&gt;&lt;em&gt;Microservice&lt;/em&gt; is a highly distracting word that serves to confuse developers, architects, and IT leaders into believing that we can actually have a utopian application architecture.&lt;/p&gt;&lt;/blockquote&gt; &lt;p&gt;Um, no. While that&amp;#8217;s cynical but true, the correct definition is:&lt;/p&gt; &lt;blockquote&gt;&lt;p&gt;&lt;em&gt;Microservices&lt;/em&gt; are an &lt;em&gt;architectural optimization&lt;/em&gt; that treats the modules of an application as independently owned and deployed services for the purpose of increasing an organization&amp;#8217;s velocity and eliminating &lt;a href="https://en.wikipedia.org/wiki/Technical_debt"&gt;technical debt&lt;/a&gt;.&lt;/p&gt;&lt;/blockquote&gt; &lt;p&gt;The performance of an IT organization has a strong correlation to business performance, boosting productivity, profitability, and market share&lt;sup&gt;2&lt;/sup&gt;. Statistics over the last few years for high performers versus low performers include moving code from commit to production &lt;em&gt;200x faster&lt;/em&gt;&lt;sup&gt;3&lt;/sup&gt;, deployments being &lt;em&gt;46x more frequent&lt;/em&gt;, and the lead time for changes being &lt;em&gt;440x faster&lt;/em&gt;&lt;sup&gt;4&lt;/sup&gt;. Technologies and techniques such as containers, automated testing, and deployment pipelines enable teams to go faster. In addition, high-performing teams are introducing errors at a much lower rate and are recovering from the errors they do create at a much higher rate.&lt;/p&gt; &lt;p&gt;When considering microservices, the goal is not to pursue a microservice architecture mindlessly. The goal is to use microservices where they make sense. Specifically, to use microservices to speed up development, to lower the risk of bad things happening, and to make it simpler to understand and recover from bad things when they happen. (Which, of course, they probably will.)&lt;/p&gt; &lt;p&gt;The case study Christian covered is the &lt;a href="https://developers.redhat.com/ticket-monster/"&gt;TicketMonster demo&lt;/a&gt;. Originally written for JBoss, the code has been around for at least a decade and has become, in Christian&amp;#8217;s words, &amp;#8220;a morass of stuff.&amp;#8221; Maintaining this or any other monolith is a major pain for a long list of reasons:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Making changes in one place negatively affects unrelated areas&lt;/li&gt; &lt;li&gt;We have low confidence that reasonable changes won&amp;#8217;t break something somewhere else&lt;/li&gt; &lt;li&gt;We spend lots of time coordinating work among team members&lt;/li&gt; &lt;li&gt;The structure of the application has eroded or is non-existent&lt;/li&gt; &lt;li&gt;We have no way to quantify how long code merges will take&lt;/li&gt; &lt;li&gt;Development is tedious because the project is so big (the IDE bogs down, running tests take forever, bootstrap times are long, etc.)&lt;/li&gt; &lt;li&gt;Changes to one module force changes across other modules&lt;/li&gt; &lt;li&gt;Sunsetting outdated technology is difficult&lt;/li&gt; &lt;li&gt;We may have to base new applications on old approaches like batch processing&lt;/li&gt; &lt;li&gt;The monolith gets in its own way when managing resources, allocations, and computations.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;All of these factors make microservices attractive. But it&amp;#8217;s important to remember that &lt;em&gt;microservices are about optimizing for speed&lt;/em&gt;. You need to ask yourself the question, &amp;#8220;Is the architecture of our application the bottleneck that keeps us from moving faster?&amp;#8221; If it&amp;#8217;s not, Christian prescribes a three-step process:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Figure out what &lt;em&gt;is&lt;/em&gt; keeping you from moving faster.&lt;/li&gt; &lt;li&gt;Fix that.&lt;/li&gt; &lt;li&gt;Come back and look at microservices again.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;If microservices are a good fit for you, the question is how to break a monolith into microservices. Some common approaches include, &amp;#8220;Do one thing and do it well,&amp;#8221; &amp;#8220;Organize around verbs,&amp;#8221; &amp;#8220;Organize around nouns,&amp;#8221; and &amp;#8220;Focus on products not projects.&amp;#8221; We said similar things about SOA back in the day, however. &amp;#8220;Services are autonomous,&amp;#8221; &amp;#8220;Loose coupling is vital,&amp;#8221; &amp;#8220;Boundaries are explicit,&amp;#8221; were equally high-minded goals that often eluded us. So we have platitudes on what the system should be, but no real guidelines on how to make it happen. A more sophisticated approach:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Identify modules and boundaries (&lt;em&gt;aka&lt;/em&gt; use &lt;a href="https://en.wikipedia.org/wiki/Domain-driven_design"&gt;domain-driven design&lt;/a&gt;)&lt;/li&gt; &lt;li&gt;Align to business capabilities&lt;/li&gt; &lt;li&gt;Identify data entities responsible for features and modules&lt;/li&gt; &lt;li&gt;Break out those entities and wrap them with APIs or services&lt;/li&gt; &lt;li&gt;Update old code to call the new APIs&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;That&amp;#8217;s all well and good, but it misses a lot of detail&lt;sup&gt;5&lt;/sup&gt;. As usual, reality rears its ugly, pointy little head, presenting difficult problems that can&amp;#8217;t be waved away:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;It&amp;#8217;s not easy to modularize a monolith. If it were, the teams that maintained the monolith over the years would have kept it modular.&lt;/li&gt; &lt;li&gt;There are often tight couplings and integrity constraints with an SQL database. This is often overlooked. The database backing the monolith probably has normalized tables and referential integrity in place. Disregarding that can be disastrous. (In fact, it almost certainly will be disastrous.)&lt;/li&gt; &lt;li&gt;It&amp;#8217;s difficult to understand which modules use which tables in the database.&lt;/li&gt; &lt;li&gt;As appealing as it might be, we can&amp;#8217;t shut down the enterprise to do migrations.&lt;/li&gt; &lt;li&gt;There will be some ugly migration steps that can&amp;#8217;t be wished away. You&amp;#8217;ll have to undo the existing technical debt accrued over the years.&lt;/li&gt; &lt;li&gt;Finally, there is probably a point of diminishing returns at which it simply doesn&amp;#8217;t make sense to break any more things out of the monolith&lt;sup&gt;6&lt;/sup&gt;.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Yet another list—you need to make sure that these things are in place:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Test coverage for the existing project&lt;sup&gt;7&lt;/sup&gt; (consider &lt;a href="http://arquillian.org"&gt;Arquillian&lt;/a&gt; for integration testing. Also consider Michael Feathers&amp;#8217; book &lt;a href="https://www.amazon.com/Working-Effectively-Legacy-Michael-Feathers/dp/0131177052"&gt;Working Effectively with Legacy Code&lt;/a&gt;.)&lt;/li&gt; &lt;li&gt;Some level of monitoring to detect issues, exceptions, etc.&lt;/li&gt; &lt;li&gt;Some level of black-box system tests and load testing in place (&lt;a href="http://jmeter.apache.org"&gt;JMeter&lt;/a&gt; and &lt;a href="https://gatling.io"&gt;Gatling&lt;/a&gt; can help here)&lt;/li&gt; &lt;li&gt;The ability to deploy to an environment reliably (OpenShift or Kubernetes)&lt;/li&gt; &lt;li&gt;Some kind of CI/CD pipeline to make changes economical&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Moving on to the sample monolith&amp;#8230;the Ticket Monster code has the UI baked in to it. If possible, a good first step is to break the UI out of the monolith. That gives you the freedom to bring the UI into the modern era, and it also sets the stage for the separation of concerns that is promised by a microservices architecture. Start by making a copy of the UI code separate from the monolith by taking out all the JavaScript, CSS, etc. The new UI simply calls the back-end systems of the monolith. You can then do a dark launch or a canary deployment of the new UI, sending only some production traffic to it as you verify that the new UI is functionally equivalent to the original.&lt;/p&gt; &lt;p&gt;With that in mind, Christian stressed the difference between deployments and releases. A &lt;em&gt;deployment&lt;/em&gt; is simply code that lives on a production server but doesn&amp;#8217;t get any traffic. It isn&amp;#8217;t considered a &lt;em&gt;release&lt;/em&gt; until it&amp;#8217;s using live data in a production environment. A deployment should be a non-event to the business. Developers should be able to deploy code without approvals or intervention by administrators or the ops staff. Deciding to route production traffic to the deployment, on the other hand, is a business decision made once the deployment has been tested thoroughly.&lt;/p&gt; &lt;p&gt;The &lt;a href="http://istio.io"&gt;Istio service mesh&lt;/a&gt; makes this easy in an OpenShift / Kubernetes environment. It is an infrastructure that allows you to separate deployments and releases. (Read Don Schenck’s excellent &lt;a href="https://developers.redhat.com/blog/2018/03/06/introduction-istio-makes-mesh-things/"&gt;Introduction to Istio blog series&lt;/a&gt; to fully understand this technology.) Istio lets you say that only 1% of the traffic should go to the new deployment or that only users who have certain characteristics should see the new deployment. Another definition:&lt;/p&gt; &lt;blockquote&gt;&lt;p&gt;A &lt;em&gt;service mesh&lt;/em&gt; is a decentralized application networking infrastructure among your services that provides resiliency, security, observability, and routing control. A service mesh is comprised of a control plane and a data plane.&lt;/p&gt;&lt;/blockquote&gt; &lt;p&gt;Istio works via &lt;em&gt;sidecar proxies&lt;/em&gt;. Every service in the service mesh has a proxy beside it. Any traffic meant for a service first goes to the sidecar proxy. Istio controls the sidecar proxy to determine how or when or if that traffic is actually delivered to the service. This approach allows you to manage all the microservices in your service mesh without changing any code..&lt;/p&gt; &lt;p&gt;Continuing the refactoring of the monolith, we can take a particular function of the monolith and move that code into a service. The starting point, once again, is to copy the code from the monolith and put it into a separate module. Then we can use Istio to route only some of the traffic to the new service. To maximize the value of our investment, the service we choose should be something that provides significant business value. In other words, we look at the monolith and determine that some part of the system could give us a substantial ROI if we made it more agile and flexible. In the Ticket Monster example, the order processing functions could be far more valuable to the business if they were in a separate module that could be enhanced independently of the monolith. For example, if that code was written a decade ago, it doesn&amp;#8217;t support things like ApplePay or Venmo. A low-risk architecture for adding new methods of payment would clearly help the business stay current.&lt;/p&gt; &lt;p&gt;To sum up:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Write lots of tests. For the monolith if you can, but definitely for the new services.&lt;/li&gt; &lt;li&gt;Use advanced techniques such as canary deployments and other fine-grained traffic control to manage the transition from deployments to releases.&lt;/li&gt; &lt;li&gt;Reduce boilerplate code for data integration in the initial service implementation.&lt;/li&gt; &lt;li&gt;Use technical debt to your advantage.&lt;/li&gt; &lt;li&gt;Have lots of monitoring in place.&lt;/li&gt; &lt;li&gt;Leverage your deployment and release infrastructure to experiment and learn about your system as you go forward.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;This was a great session with lots of practical advice based on real-world experience. If you&amp;#8217;d like to get it from the source, the &lt;a href="https://youtu.be/YP6wJXblyWM"&gt;video of Christian&amp;#8217;s presentation&lt;/a&gt; is one of the &lt;a href="https://developers.redhat.com/blog/2018/05/15/100-red-hat-summit-2018-session-videos-online/"&gt;100+ Red Hat Summit 2018 breakout sessions&lt;/a&gt; you can view online for free.&lt;/p&gt; &lt;div align="center"&gt;&lt;iframe src="https://www.youtube.com/embed/YP6wJXblyWM" width="560" height="315" frameborder="0" allowfullscreen="allowfullscreen"&gt;&lt;span data-mce-type="bookmark" style="display: inline-block; width: 0px; overflow: hidden; line-height: 0;" class="mce_SELRES_start"&gt;﻿&lt;/span&gt;&lt;/iframe&gt;&lt;/div&gt; &lt;p&gt;Good luck with your enterprise modernization!&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Resources:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://www.slideshare.net/ceposta/lowering-the-risk-of-monolith-to-microservices"&gt;Christian&amp;#8217;s slides for this presentation on SlideShare&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Christian and Burr Sutter&amp;#8217;s new book: &lt;a href="https://developers.redhat.com/books/introducing-istio-service-mesh-microservices/"&gt;Introducing Istio Service Mesh for Microservices&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Christian&amp;#8217;s book: &lt;a href="https://developers.redhat.com/books/microservices-java-developers-hands-introduction-frameworks-and-containers/"&gt;Microservices for Java Developers: A Hands-on Introduction to Frameworks and Containers&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/books/microservices-java-developers-hands-introduction-frameworks-and-containers/"&gt;Christian&amp;#8217;s blog articles&lt;/a&gt; on &lt;a href="https://developers.redhat.com/blog/"&gt;developers.redhat.com/blog&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;hr /&gt; &lt;p&gt;&lt;strong&gt;Footnotes:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;sup&gt;1&lt;/sup&gt; Fictitious, but based on a true story. Many of them, in fact.&lt;/p&gt; &lt;p&gt;&lt;sup&gt;2&lt;/sup&gt; See &lt;a href="https://puppet.com/resources/whitepaper/2014-state-devops-report"&gt;https://puppet.com/resources/whitepaper/2014-state-devops-report&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;&lt;sup&gt;3&lt;/sup&gt; See &lt;a href="https://puppet.com/resources/whitepaper/2015-state-devops-report"&gt;https://puppet.com/resources/whitepaper/2015-state-devops-report&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;&lt;sup&gt;4&lt;/sup&gt; See &lt;a href="https://puppet.com/resources/whitepaper/2017-state-devops-report"&gt;https://puppet.com/resources/whitepaper/2017-state-of-devops-report&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;&lt;sup&gt;5&lt;/sup&gt; What, you were expecting a full, prescriptive recommendation from a blog post? Sorry, but the real world is more complicated than that.&lt;/p&gt; &lt;p&gt;&lt;sup&gt;6&lt;/sup&gt; You can stab it with your steely knives, but you just can&amp;#8217;t kill the beast.&lt;/p&gt; &lt;p&gt;&lt;sup&gt;7&lt;/sup&gt; One definition of legacy code is &amp;#8220;code that doesn&amp;#8217;t have any tests.&amp;#8221;’&lt;/p&gt; &lt;p&gt;&lt;a class="a2a_button_facebook" href="https://www.addtoany.com/add_to/facebook?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2018%2F05%2F21%2Fred-hat-summit-lowering-the-risk-of-monolith-to-microservices%2F&amp;#38;linkname=Red%20Hat%20Summit%3A%20Lowering%20the%20risk%20of%20monolith%20to%20microservices" title="Facebook" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_twitter" href="https://www.addtoany.com/add_to/twitter?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2018%2F05%2F21%2Fred-hat-summit-lowering-the-risk-of-monolith-to-microservices%2F&amp;#38;linkname=Red%20Hat%20Summit%3A%20Lowering%20the%20risk%20of%20monolith%20to%20microservices" title="Twitter" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_google_plus" href="https://www.addtoany.com/add_to/google_plus?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2018%2F05%2F21%2Fred-hat-summit-lowering-the-risk-of-monolith-to-microservices%2F&amp;#38;linkname=Red%20Hat%20Summit%3A%20Lowering%20the%20risk%20of%20monolith%20to%20microservices" title="Google+" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_linkedin" href="https://www.addtoany.com/add_to/linkedin?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2018%2F05%2F21%2Fred-hat-summit-lowering-the-risk-of-monolith-to-microservices%2F&amp;#38;linkname=Red%20Hat%20Summit%3A%20Lowering%20the%20risk%20of%20monolith%20to%20microservices" title="LinkedIn" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_email" href="https://www.addtoany.com/add_to/email?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2018%2F05%2F21%2Fred-hat-summit-lowering-the-risk-of-monolith-to-microservices%2F&amp;#38;linkname=Red%20Hat%20Summit%3A%20Lowering%20the%20risk%20of%20monolith%20to%20microservices" title="Email" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_print" href="https://www.addtoany.com/add_to/print?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2018%2F05%2F21%2Fred-hat-summit-lowering-the-risk-of-monolith-to-microservices%2F&amp;#38;linkname=Red%20Hat%20Summit%3A%20Lowering%20the%20risk%20of%20monolith%20to%20microservices" title="Print" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_reddit" href="https://www.addtoany.com/add_to/reddit?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2018%2F05%2F21%2Fred-hat-summit-lowering-the-risk-of-monolith-to-microservices%2F&amp;#38;linkname=Red%20Hat%20Summit%3A%20Lowering%20the%20risk%20of%20monolith%20to%20microservices" title="Reddit" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_flipboard" href="https://www.addtoany.com/add_to/flipboard?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2018%2F05%2F21%2Fred-hat-summit-lowering-the-risk-of-monolith-to-microservices%2F&amp;#38;linkname=Red%20Hat%20Summit%3A%20Lowering%20the%20risk%20of%20monolith%20to%20microservices" title="Flipboard" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_dd addtoany_share_save addtoany_share" href="https://www.addtoany.com/share#url=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2018%2F05%2F21%2Fred-hat-summit-lowering-the-risk-of-monolith-to-microservices%2F&amp;#38;title=Red%20Hat%20Summit%3A%20Lowering%20the%20risk%20of%20monolith%20to%20microservices" data-a2a-url="https://developers.redhat.com/blog/2018/05/21/red-hat-summit-lowering-the-risk-of-monolith-to-microservices/" data-a2a-title="Red Hat Summit: Lowering the risk of monolith to microservices"&gt;&lt;img src="https://static.addtoany.com/buttons/favicon.png" alt="Share"&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;The post &lt;a rel="nofollow" href="https://developers.redhat.com/blog/2018/05/21/red-hat-summit-lowering-the-risk-of-monolith-to-microservices/"&gt;Red Hat Summit: Lowering the risk of monolith to microservices&lt;/a&gt; appeared first on &lt;a rel="nofollow" href="https://developers.redhat.com/blog"&gt;RHD Blog&lt;/a&gt;.&lt;/p&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/fz4rA-JfAIc" height="1" width="1" alt=""/&gt;</content><summary>Christian Posta, Chief Architect at Red Hat, presented the story of a fictitious company1 moving a monolithic application to microservices. When considering risk, we think we know the bad things that can happen and the probabilities of those bad things actually happening. Christian defines a monolith as a large application developed over many years by different teams that delivers proven business ...</summary><dc:creator>unknown</dc:creator><dc:date>2018-05-21T17:57:33Z</dc:date><feedburner:origLink>https://developers.redhat.com/blog/2018/05/21/red-hat-summit-lowering-the-risk-of-monolith-to-microservices/</feedburner:origLink></entry><entry><title>Apache Camel URI completion: easy installation for Eclipse, VS Code, and OpenShift.io</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/GTWE1fg4ESg/" /><category term="apache camel" scheme="searchisko:content:tags" /><category term="Developer Tools" scheme="searchisko:content:tags" /><category term="Eclipse" scheme="searchisko:content:tags" /><category term="feed_group_name_nonmiddleware" scheme="searchisko:content:tags" /><category term="feed_name_redhat_developer_blog" scheme="searchisko:content:tags" /><category term="FUSE" scheme="searchisko:content:tags" /><category term="Fuse Tooling" scheme="searchisko:content:tags" /><category term="ide" scheme="searchisko:content:tags" /><category term="JBoss Fuse" scheme="searchisko:content:tags" /><category term="language servers" scheme="searchisko:content:tags" /><category term="OpenShift.io" scheme="searchisko:content:tags" /><category term="Red Hat Fuse" scheme="searchisko:content:tags" /><category term="Red Hat OpenShift.io" scheme="searchisko:content:tags" /><category term="VS Code" scheme="searchisko:content:tags" /><author><name>Aurélien Pupier</name></author><id>searchisko:content:id:jbossorg_blog-apache_camel_uri_completion_easy_installation_for_eclipse_vs_code_and_openshift_io</id><updated>2018-05-21T11:00:34Z</updated><published>2018-05-21T11:00:34Z</published><content type="html">&lt;p&gt;Discoverability and ease of installation of Apache Camel tooling based on the Language Server Protocol has been improved. Manual download and installation of binaries is no longer necessary!  For the Eclipse desktop IDE and the VS Code environment you can now find and install the Camel tooling directly from the marketplaces for each development environment.&lt;/p&gt; &lt;p&gt;Camel Language Server is now also available in &lt;a href="http://openshift.io/"&gt;Red Hat OpenShift.io!&lt;/a&gt;&lt;/p&gt; &lt;p&gt;In this article, I will show you how you can install Camel tooling via the marketplaces for Eclipse and VS Code.  I will also show how to enable Camel tooling in your OpenShift.io workspace.&lt;span id="more-494857"&gt;&lt;/span&gt;&lt;/p&gt; &lt;h2&gt;Eclipse and VS Code Marketplaces&lt;/h2&gt; &lt;p&gt;The Camel tooling is available on the &lt;a href="https://marketplace.eclipse.org/content/apache-camel-language-server"&gt;Eclipse marketplace&lt;/a&gt;. This means that you can discover and install it directly from your &lt;a href="https://www.eclipse.org/ide/"&gt;Eclipse Desktop IDE&lt;/a&gt;:&lt;/p&gt; &lt;p&gt;&lt;img class=" aligncenter wp-image-494867 " data-add-featherlight="https://developers.redhat.com/blog/wp-content/uploads/2018/05/EclipseSearchCamel.png" src="https://developers.redhat.com/blog/wp-content/uploads/2018/05/EclipseSearchCamel.png" alt="Language Support for Apache Camel extension entry displayed when searching for Camel in Eclipse Marketplace" width="811" height="951" srcset="https://developers.redhat.com/blog/wp-content/uploads/2018/05/EclipseSearchCamel.png 811w, https://developers.redhat.com/blog/wp-content/uploads/2018/05/EclipseSearchCamel-256x300.png 256w, https://developers.redhat.com/blog/wp-content/uploads/2018/05/EclipseSearchCamel-768x901.png 768w" sizes="(max-width: 811px) 100vw, 811px" /&gt;&lt;/p&gt; &lt;p&gt;The tooling is also available on &lt;a href="https://marketplace.visualstudio.com/items?itemName=camel-tooling.vscode-apache-camel"&gt;VS Code marketplace&lt;/a&gt;. This means that you can discover and install it directly from your &lt;a href="https://code.visualstudio.com/"&gt;VS Code IDE&lt;/a&gt;:&lt;/p&gt; &lt;p&gt;&lt;img class=" aligncenter wp-image-494877 " data-add-featherlight="https://developers.redhat.com/blog/wp-content/uploads/2018/05/VSCodeSearchCamel-1024x639.png" src="https://developers.redhat.com/blog/wp-content/uploads/2018/05/VSCodeSearchCamel.png" alt="Language Support for Apache Camel extension entry displayed when searching for Camel in VS Code Extension manager" width="1086" height="678" srcset="https://developers.redhat.com/blog/wp-content/uploads/2018/05/VSCodeSearchCamel.png 1086w, https://developers.redhat.com/blog/wp-content/uploads/2018/05/VSCodeSearchCamel-300x187.png 300w, https://developers.redhat.com/blog/wp-content/uploads/2018/05/VSCodeSearchCamel-768x479.png 768w, https://developers.redhat.com/blog/wp-content/uploads/2018/05/VSCodeSearchCamel-1024x639.png 1024w" sizes="(max-width: 1086px) 100vw, 1086px" /&gt;&lt;/p&gt; &lt;h2&gt;Use in OpenShift.io&lt;/h2&gt; &lt;p&gt;Camel Language Server is now also available in &lt;a href="http://openshift.io/"&gt;OpenShift.io!&lt;/a&gt; It requires a workspace configuration to be turned on.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Go to your workspace&lt;br /&gt; &lt;img class=" aligncenter wp-image-494957 " data-add-featherlight="https://developers.redhat.com/blog/wp-content/uploads/2018/05/1-openShiftIOWorkspace-1024x555.png" src="https://developers.redhat.com/blog/wp-content/uploads/2018/05/1-openShiftIOWorkspace.png" alt="" srcset="https://developers.redhat.com/blog/wp-content/uploads/2018/05/1-openShiftIOWorkspace.png 1920w, https://developers.redhat.com/blog/wp-content/uploads/2018/05/1-openShiftIOWorkspace-300x163.png 300w, https://developers.redhat.com/blog/wp-content/uploads/2018/05/1-openShiftIOWorkspace-768x416.png 768w, https://developers.redhat.com/blog/wp-content/uploads/2018/05/1-openShiftIOWorkspace-1024x555.png 1024w" sizes="(max-width: 1920px) 100vw, 1920px" /&gt;&lt;/li&gt; &lt;li&gt;Click on the yellow top left arrow&lt;img class=" aligncenter wp-image-494967 " data-add-featherlight="https://developers.redhat.com/blog/wp-content/uploads/2018/05/2-openShiftIOWorkspacePanel-1024x555.png" src="https://developers.redhat.com/blog/wp-content/uploads/2018/05/2-openShiftIOWorkspacePanel.png" alt="" srcset="https://developers.redhat.com/blog/wp-content/uploads/2018/05/2-openShiftIOWorkspacePanel.png 1920w, https://developers.redhat.com/blog/wp-content/uploads/2018/05/2-openShiftIOWorkspacePanel-300x163.png 300w, https://developers.redhat.com/blog/wp-content/uploads/2018/05/2-openShiftIOWorkspacePanel-768x416.png 768w, https://developers.redhat.com/blog/wp-content/uploads/2018/05/2-openShiftIOWorkspacePanel-1024x555.png 1024w" sizes="(max-width: 1920px) 100vw, 1920px" /&gt;&lt;/li&gt; &lt;li&gt;Click on &amp;#8220;Workspaces&amp;#8221;&lt;img class=" aligncenter wp-image-494977 " data-add-featherlight="https://developers.redhat.com/blog/wp-content/uploads/2018/05/3-openShiftIOWorkspaces-1024x555.png" src="https://developers.redhat.com/blog/wp-content/uploads/2018/05/3-openShiftIOWorkspaces.png" alt="" srcset="https://developers.redhat.com/blog/wp-content/uploads/2018/05/3-openShiftIOWorkspaces.png 1920w, https://developers.redhat.com/blog/wp-content/uploads/2018/05/3-openShiftIOWorkspaces-300x163.png 300w, https://developers.redhat.com/blog/wp-content/uploads/2018/05/3-openShiftIOWorkspaces-768x416.png 768w, https://developers.redhat.com/blog/wp-content/uploads/2018/05/3-openShiftIOWorkspaces-1024x555.png 1024w" sizes="(max-width: 1920px) 100vw, 1920px" /&gt;&lt;/li&gt; &lt;li&gt;Click on the &amp;#8220;configuration icon corresponding to your workspace&lt;img class=" aligncenter wp-image-494987 " data-add-featherlight="https://developers.redhat.com/blog/wp-content/uploads/2018/05/4-openShiftIOConfigurationWorkspace-1024x555.png" src="https://developers.redhat.com/blog/wp-content/uploads/2018/05/4-openShiftIOConfigurationWorkspace.png" alt="" srcset="https://developers.redhat.com/blog/wp-content/uploads/2018/05/4-openShiftIOConfigurationWorkspace.png 1920w, https://developers.redhat.com/blog/wp-content/uploads/2018/05/4-openShiftIOConfigurationWorkspace-300x163.png 300w, https://developers.redhat.com/blog/wp-content/uploads/2018/05/4-openShiftIOConfigurationWorkspace-768x416.png 768w, https://developers.redhat.com/blog/wp-content/uploads/2018/05/4-openShiftIOConfigurationWorkspace-1024x555.png 1024w" sizes="(max-width: 1920px) 100vw, 1920px" /&gt;&lt;/li&gt; &lt;li&gt;Click on &amp;#8220;Installers&amp;#8221; menu and turn on &amp;#8220;Apache Camel language server&lt;img class=" aligncenter wp-image-494997 " data-add-featherlight="https://developers.redhat.com/blog/wp-content/uploads/2018/05/5-turnOnCamelLanguageServer-1024x555.png" src="https://developers.redhat.com/blog/wp-content/uploads/2018/05/5-turnOnCamelLanguageServer.png" alt="" srcset="https://developers.redhat.com/blog/wp-content/uploads/2018/05/5-turnOnCamelLanguageServer.png 1920w, https://developers.redhat.com/blog/wp-content/uploads/2018/05/5-turnOnCamelLanguageServer-300x163.png 300w, https://developers.redhat.com/blog/wp-content/uploads/2018/05/5-turnOnCamelLanguageServer-768x416.png 768w, https://developers.redhat.com/blog/wp-content/uploads/2018/05/5-turnOnCamelLanguageServer-1024x555.png 1024w" sizes="(max-width: 1920px) 100vw, 1920px" /&gt;&lt;/li&gt; &lt;li&gt;Apply and open the workspace&lt;/li&gt; &lt;li&gt;Reopen the xml file&lt;/li&gt; &lt;li&gt;Enjoy!&lt;br /&gt; &lt;img class=" aligncenter wp-image-495007 " data-add-featherlight="https://developers.redhat.com/blog/wp-content/uploads/2018/05/completionOpenShift.io_-1024x555.png" src="https://developers.redhat.com/blog/wp-content/uploads/2018/05/completionOpenShift.io_.png" alt="" srcset="https://developers.redhat.com/blog/wp-content/uploads/2018/05/completionOpenShift.io_.png 1920w, https://developers.redhat.com/blog/wp-content/uploads/2018/05/completionOpenShift.io_-300x163.png 300w, https://developers.redhat.com/blog/wp-content/uploads/2018/05/completionOpenShift.io_-768x416.png 768w, https://developers.redhat.com/blog/wp-content/uploads/2018/05/completionOpenShift.io_-1024x555.png 1024w" sizes="(max-width: 1920px) 100vw, 1920px" /&gt;&lt;/li&gt; &lt;/ul&gt; &lt;h2&gt;New home for project code&lt;/h2&gt; &lt;p&gt;Please note that the Camel Language Server has a new home. It is now co-hosted with the &lt;a href="https://github.com/camel-tooling/camel-idea-plugin"&gt;Camel Idea plugin&lt;/a&gt;. You can find it here: &lt;a href="https://github.com/camel-tooling/camel-language-server"&gt;github.com/camel-tooling/camel-language-server&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Now that it is easily installable, it is time to give it a try!&lt;/p&gt; &lt;p&gt;&lt;a class="a2a_button_facebook" href="https://www.addtoany.com/add_to/facebook?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2018%2F05%2F21%2Fapache-camel-uri-completion-easy-installation-for-eclipse-vs-code-and-openshift-io%2F&amp;#38;linkname=Apache%20Camel%20URI%20completion%3A%20easy%20installation%20for%20Eclipse%2C%20VS%20Code%2C%20and%20OpenShift.io" title="Facebook" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_twitter" href="https://www.addtoany.com/add_to/twitter?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2018%2F05%2F21%2Fapache-camel-uri-completion-easy-installation-for-eclipse-vs-code-and-openshift-io%2F&amp;#38;linkname=Apache%20Camel%20URI%20completion%3A%20easy%20installation%20for%20Eclipse%2C%20VS%20Code%2C%20and%20OpenShift.io" title="Twitter" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_google_plus" href="https://www.addtoany.com/add_to/google_plus?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2018%2F05%2F21%2Fapache-camel-uri-completion-easy-installation-for-eclipse-vs-code-and-openshift-io%2F&amp;#38;linkname=Apache%20Camel%20URI%20completion%3A%20easy%20installation%20for%20Eclipse%2C%20VS%20Code%2C%20and%20OpenShift.io" title="Google+" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_linkedin" href="https://www.addtoany.com/add_to/linkedin?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2018%2F05%2F21%2Fapache-camel-uri-completion-easy-installation-for-eclipse-vs-code-and-openshift-io%2F&amp;#38;linkname=Apache%20Camel%20URI%20completion%3A%20easy%20installation%20for%20Eclipse%2C%20VS%20Code%2C%20and%20OpenShift.io" title="LinkedIn" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_email" href="https://www.addtoany.com/add_to/email?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2018%2F05%2F21%2Fapache-camel-uri-completion-easy-installation-for-eclipse-vs-code-and-openshift-io%2F&amp;#38;linkname=Apache%20Camel%20URI%20completion%3A%20easy%20installation%20for%20Eclipse%2C%20VS%20Code%2C%20and%20OpenShift.io" title="Email" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_print" href="https://www.addtoany.com/add_to/print?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2018%2F05%2F21%2Fapache-camel-uri-completion-easy-installation-for-eclipse-vs-code-and-openshift-io%2F&amp;#38;linkname=Apache%20Camel%20URI%20completion%3A%20easy%20installation%20for%20Eclipse%2C%20VS%20Code%2C%20and%20OpenShift.io" title="Print" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_reddit" href="https://www.addtoany.com/add_to/reddit?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2018%2F05%2F21%2Fapache-camel-uri-completion-easy-installation-for-eclipse-vs-code-and-openshift-io%2F&amp;#38;linkname=Apache%20Camel%20URI%20completion%3A%20easy%20installation%20for%20Eclipse%2C%20VS%20Code%2C%20and%20OpenShift.io" title="Reddit" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_flipboard" href="https://www.addtoany.com/add_to/flipboard?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2018%2F05%2F21%2Fapache-camel-uri-completion-easy-installation-for-eclipse-vs-code-and-openshift-io%2F&amp;#38;linkname=Apache%20Camel%20URI%20completion%3A%20easy%20installation%20for%20Eclipse%2C%20VS%20Code%2C%20and%20OpenShift.io" title="Flipboard" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_dd addtoany_share_save addtoany_share" href="https://www.addtoany.com/share#url=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2018%2F05%2F21%2Fapache-camel-uri-completion-easy-installation-for-eclipse-vs-code-and-openshift-io%2F&amp;#38;title=Apache%20Camel%20URI%20completion%3A%20easy%20installation%20for%20Eclipse%2C%20VS%20Code%2C%20and%20OpenShift.io" data-a2a-url="https://developers.redhat.com/blog/2018/05/21/apache-camel-uri-completion-easy-installation-for-eclipse-vs-code-and-openshift-io/" data-a2a-title="Apache Camel URI completion: easy installation for Eclipse, VS Code, and OpenShift.io"&gt;&lt;img src="https://static.addtoany.com/buttons/favicon.png" alt="Share"&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;The post &lt;a rel="nofollow" href="https://developers.redhat.com/blog/2018/05/21/apache-camel-uri-completion-easy-installation-for-eclipse-vs-code-and-openshift-io/"&gt;Apache Camel URI completion: easy installation for Eclipse, VS Code, and OpenShift.io&lt;/a&gt; appeared first on &lt;a rel="nofollow" href="https://developers.redhat.com/blog"&gt;RHD Blog&lt;/a&gt;.&lt;/p&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/GTWE1fg4ESg" height="1" width="1" alt=""/&gt;</content><summary>Discoverability and ease of installation of Apache Camel tooling based on the Language Server Protocol has been improved. Manual download and installation of binaries is no longer necessary!  For the Eclipse desktop IDE and the VS Code environment you can now find and install the Camel tooling directly from the marketplaces for each development environment. Camel Language Server is now also availa...</summary><dc:creator>Aurélien Pupier</dc:creator><dc:date>2018-05-21T11:00:34Z</dc:date><feedburner:origLink>https://developers.redhat.com/blog/2018/05/21/apache-camel-uri-completion-easy-installation-for-eclipse-vs-code-and-openshift-io/</feedburner:origLink></entry><entry><title>Contract Net Protocol with jBPM</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/JD6HGzap7OE/contract-net-protocol-with-jbpm.html" /><category term="case_management" scheme="searchisko:content:tags" /><category term="contract_net" scheme="searchisko:content:tags" /><category term="contract_net_protocol" scheme="searchisko:content:tags" /><category term="feed_group_name_jbossjbpmcommunity" scheme="searchisko:content:tags" /><category term="feed_name_swiderskimaciej" scheme="searchisko:content:tags" /><category term="jbpm7" scheme="searchisko:content:tags" /><author><name>Maciej Swiderski</name></author><id>searchisko:content:id:jbossorg_blog-contract_net_protocol_with_jbpm</id><updated>2018-05-21T10:08:38Z</updated><published>2018-05-21T09:25:00Z</published><content type="html">&lt;div dir="ltr" style="text-align: left;" trbidi="on"&gt;jBPM provides lots of capabilities that could be used out of the box to build rather sophisticated solutions. In this article I'd like to show one of them - &lt;a href="https://en.wikipedia.org/wiki/Contract_Net_Protocol"&gt;Contract Net Protocol&lt;/a&gt;.&lt;br /&gt;&lt;br /&gt;&lt;i&gt;Contract Net Protocol (CNP) is a task-sharing protocol in multi-agent systems, consisting of a collection of nodes or software agents that form the 'contract net'. Each node on the network can, at different times or for different tasks, be a manager or a contractor.&lt;/i&gt; [1]&lt;br /&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;This concepts nicely fits into case management capabilities of jBPM 7. It allows to easily model interaction between Initiator and Participant(s).&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto; text-align: center;"&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style="text-align: center;"&gt;&lt;a href="https://1.bp.blogspot.com/-A2afY42VoYk/WwKHw1qALLI/AAAAAAAABcg/frKuLQWY9c8c_sAM6w9uU5fB1kDDnLkGQCLcBGAs/s1600/image001.gif.jpeg" imageanchor="1" style="margin-left: auto; margin-right: auto;"&gt;&lt;img border="0" data-original-height="477" data-original-width="348" height="640" src="https://1.bp.blogspot.com/-A2afY42VoYk/WwKHw1qALLI/AAAAAAAABcg/frKuLQWY9c8c_sAM6w9uU5fB1kDDnLkGQCLcBGAs/s640/image001.gif.jpeg" width="466" /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class="tr-caption" style="text-align: center;"&gt;source&amp;nbsp;http://www.fipa.org/specs/fipa00029/SC00029H.html#_ftnref1&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;div&gt;Contract can be announced to many participants (aka bidders) that can either be interested in the contract and then bid or simply reject it and leave the contract net completely.&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;with jBPM 7, contract net can be modelled as a case definition where individual phases of the protocol can be externalised to processes to carry on with additional work. This improves readability and at the same time promotes reusability of the implementation.&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div class="separator" style="clear: both; text-align: center;"&gt;&lt;a href="https://4.bp.blogspot.com/-1tlhyUC8e5s/WwKIuV7rhAI/AAAAAAAABco/jAL2m5Sts58hhZnmjezZ__gHw_B0nN6dwCLcBGAs/s1600/Screen%2BShot%2B2018-05-21%2Bat%2B10.51.46.png" imageanchor="1" style="margin-left: 1em; margin-right: 1em;"&gt;&lt;img border="0" data-original-height="893" data-original-width="1600" height="356" src="https://4.bp.blogspot.com/-1tlhyUC8e5s/WwKIuV7rhAI/AAAAAAAABco/jAL2m5Sts58hhZnmjezZ__gHw_B0nN6dwCLcBGAs/s640/Screen%2BShot%2B2018-05-21%2Bat%2B10.51.46.png" width="640" /&gt;&lt;/a&gt;&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;Announce contract and Offer contract are separate processes that can be implemented separately according to needs. For this basic showcase they are based on human decision and look as follows&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div class="separator" style="clear: both; text-align: center;"&gt;&lt;a href="https://1.bp.blogspot.com/-ucI_W_1IR5w/WwKJHNOdNrI/AAAAAAAABc0/WQXBoD4HIS8SoPNjjvz0VuDeqZUjtb_hwCLcBGAs/s1600/Screen%2BShot%2B2018-05-21%2Bat%2B10.53.32.png" imageanchor="1" style="margin-left: 1em; margin-right: 1em;"&gt;&lt;img border="0" data-original-height="672" data-original-width="1268" height="338" src="https://1.bp.blogspot.com/-ucI_W_1IR5w/WwKJHNOdNrI/AAAAAAAABc0/WQXBoD4HIS8SoPNjjvz0VuDeqZUjtb_hwCLcBGAs/s640/Screen%2BShot%2B2018-05-21%2Bat%2B10.53.32.png" width="640" /&gt;&lt;/a&gt;&lt;/div&gt;&lt;div&gt;Each of the participants of the contract net will have dedicated instance of the announce contract process. Each should make the decision if they will place a bid or not. In case they won't do it at all, main contract net case definition keeps a timer event on them to remove given bidder if the deadline was reached.&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;As soon as all bidders replied (or time for reply elapsed) there are set of business rules that will evaluate all provided bids and select only one. Once it is selected an Offer contract subprocess will be initiated - after milestone of selecting a bid is completed.&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div class="separator" style="clear: both; text-align: center;"&gt;&lt;a href="https://3.bp.blogspot.com/-luo3cbMmVus/WwKM0xTZFYI/AAAAAAAABdA/uKpuMHwogDoO19iExbRNFWZGyHRWsnrggCLcBGAs/s1600/Screen%2BShot%2B2018-05-21%2Bat%2B11.09.23.png" imageanchor="1" style="margin-left: 1em; margin-right: 1em;"&gt;&lt;img border="0" data-original-height="718" data-original-width="1260" height="364" src="https://3.bp.blogspot.com/-luo3cbMmVus/WwKM0xTZFYI/AAAAAAAABdA/uKpuMHwogDoO19iExbRNFWZGyHRWsnrggCLcBGAs/s640/Screen%2BShot%2B2018-05-21%2Bat%2B11.09.23.png" width="640" /&gt;&lt;/a&gt;&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;So the bidder who placed the selected bid will get the "Work on contract" task assigned to actually perform the work. Once done the worker indicates if the work was done or she failed at doing the work. In case of successful completion of the work additional business rules are invoked to verify it.&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;Completion of the work (assuming it was done) will show the results of the work to the initiator for final verification. Once the results are reviewed the contract is ended - by that case instance is ready to be closed.&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;All this in action can be seen in the following screencast&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;iframe allow="autoplay; encrypted-media" allowfullscreen="" frameborder="0" height="315" src="https://www.youtube.com/embed/vau8bxAZYQ0" width="560"&gt;&lt;/iframe&gt; &lt;br /&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;&lt;br /&gt;Again, this is just basic implementation but shows the potential that can be unleashed to build advanced Contract Net Protocol solutions.&lt;br /&gt;&lt;br /&gt;&lt;b&gt;Complete project that can be easily imported into workbench and executed in KIE server can be found &lt;a href="https://github.com/mswiderski/contract-net-example"&gt;here&lt;/a&gt;.&lt;/b&gt;&lt;/div&gt;&lt;div&gt;&lt;br /&gt;To start the case instance you can use following payload that includes data (both contract and bidders) and case role assignments&lt;br /&gt;&lt;br /&gt;&lt;pre class="brush:java"&gt;{&lt;br /&gt; "case-data": &lt;br /&gt; {&lt;br /&gt; "contract": &lt;br /&gt; {&lt;br /&gt; "Contract": &lt;br /&gt; {&lt;br /&gt; "name" : "jBPM contract",&lt;br /&gt; "description" : "provide development expertise for jBPM project",&lt;br /&gt; "price" : 1234.40&lt;br /&gt; }&lt;br /&gt; },&lt;br /&gt; "bidders": [&lt;br /&gt; {&lt;br /&gt; "Bidder": &lt;br /&gt; {&lt;br /&gt; "id" : "maciek",&lt;br /&gt; "name" : "Maciej Swiderski",&lt;br /&gt; "email" : "maciek@email.com" &lt;br /&gt; }&lt;br /&gt; },&lt;br /&gt; {&lt;br /&gt; "Bidder": &lt;br /&gt; {&lt;br /&gt; "id" : "john",&lt;br /&gt; "name" : "John doe",&lt;br /&gt; "email" : "john@email.com" &lt;br /&gt; }&lt;br /&gt; }&lt;br /&gt; ]&lt;br /&gt; },&lt;br /&gt;&lt;br /&gt; "case-user-assignments": &lt;br /&gt; {&lt;br /&gt; "Initiator": "mary",&lt;br /&gt; "Participant": "john"&lt;br /&gt; },&lt;br /&gt;&lt;br /&gt; "case-group-assignments": &lt;br /&gt; {&lt;br /&gt; &lt;br /&gt; }&lt;br /&gt;}&lt;br /&gt;&lt;br /&gt;&lt;/pre&gt;If you need more bidders just add them by copying the single bidder in the payload&lt;br /&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;[1] Source - https://en.wikipedia.org/wiki/Contract_Net_Protocol&lt;/div&gt;&lt;/div&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/JD6HGzap7OE" height="1" width="1" alt=""/&gt;</content><summary>jBPM provides lots of capabilities that could be used out of the box to build rather sophisticated solutions. In this article I'd like to show one of them - Contract Net Protocol. Contract Net Protocol (CNP) is a task-sharing protocol in multi-agent systems, consisting of a collection of nodes or software agents that form the 'contract net'. Each node on the network can, at different times or for ...</summary><dc:creator>Maciej Swiderski</dc:creator><dc:date>2018-05-21T09:25:00Z</dc:date><feedburner:origLink>http://mswiderski.blogspot.com/2018/05/contract-net-protocol-with-jbpm.html</feedburner:origLink></entry><entry><title>Get a Signed Copy of Effective Business Process Management with JBoss BPM</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/P_kEC_QYBEg/get-a-signed-copy-of-effective-business-process-management-with-jboss-bpm.html" /><category term="AppDev" scheme="searchisko:content:tags" /><category term="Automate" scheme="searchisko:content:tags" /><category term="BPM Suite" scheme="searchisko:content:tags" /><category term="bpmPaaS" scheme="searchisko:content:tags" /><category term="BRMS" scheme="searchisko:content:tags" /><category term="cloud" scheme="searchisko:content:tags" /><category term="conference" scheme="searchisko:content:tags" /><category term="feed_group_name_global" scheme="searchisko:content:tags" /><category term="feed_name_ericschabell" scheme="searchisko:content:tags" /><category term="JBoss" scheme="searchisko:content:tags" /><category term="jBPM" scheme="searchisko:content:tags" /><category term="jBPMMigration" scheme="searchisko:content:tags" /><category term="openshift" scheme="searchisko:content:tags" /><category term="Publishing" scheme="searchisko:content:tags" /><category term="xpaas" scheme="searchisko:content:tags" /><author><name>Eric D. Schabell</name></author><id>searchisko:content:id:jbossorg_blog-get_a_signed_copy_of_effective_business_process_management_with_jboss_bpm</id><updated>2018-05-21T05:00:09Z</updated><published>2018-05-21T05:00:00Z</published><content type="html">&lt;div dir="ltr" style="text-align: left;" trbidi="on"&gt;&lt;div class="separator" style="clear: both; text-align: center;"&gt;&lt;a href="https://1.bp.blogspot.com/-kfTesgjhk60/Wv7Bh9n8gaI/AAAAAAAAsqY/pbHNxQCD9yErdvRrbQx-D8nzWjHizOMkwCLcBGAs/s1600/IMG_9982.JPG" imageanchor="1" style="clear: left; float: left; margin-bottom: 1em; margin-right: 1em;"&gt;&lt;img alt="effective business process management with jboss bpm" border="0" data-original-height="1600" data-original-width="900" height="200" src="https://1.bp.blogspot.com/-kfTesgjhk60/Wv7Bh9n8gaI/AAAAAAAAsqY/pbHNxQCD9yErdvRrbQx-D8nzWjHizOMkwCLcBGAs/s200/IMG_9982.JPG" title="" width="112" /&gt;&lt;/a&gt;&lt;/div&gt;As &lt;a href="http://www.schabell.org/2018/04/book-signing-effective-bpm-with-jboss-bpm.html" target="_blank"&gt;mentioned previously&lt;/a&gt;, a few weeks ago in San Francisco, CA I spent some time signing copies of my book &lt;a href="https://developers.redhat.com/books/effective-business-process-management-jboss-bpm/?sc_cid=701f2000000tz4AAAQ" target="_blank"&gt;Effective Business Process Management with JBoss BPM&lt;/a&gt; for attendees of the &lt;a href="https://www.redhat.com/en/summit/2018?sc_cid=701f2000000tutSAAQ" target="_blank"&gt;Red Hat Summit&lt;/a&gt;.&lt;br /&gt;&lt;br /&gt;Since the book was launched as a free ebook download on &lt;a href="https://developers.redhat.com/books/effective-business-process-management-jboss-bpm/?sc_cid=701f2000000tz4AAAQ" target="_blank"&gt;Red Hat Developers&lt;/a&gt;, it was not really possible to get your hands on a real paper copy of the book.&lt;br /&gt;&lt;br /&gt;Until now.&lt;br /&gt;&lt;br /&gt;With over 200 put in print for Red Hat Summit, it was amazing to see the lines of people wanting to meet, greet and obtain their signed copies.&lt;br /&gt;&lt;br /&gt;If you would like to receive a copy, possibly signed by me, read further.&lt;br /&gt;&lt;a name='more'&gt;&lt;/a&gt;&lt;br /&gt;&lt;h3 style="text-align: left;"&gt;&lt;a href="https://1.bp.blogspot.com/-ih_789OK1cM/WqEPoyQVptI/AAAAAAAAr7k/-FobkkL2a8Iv295xSqvKaUEatf2j1LxfwCPcBGAYYCw/s1600/Schabell_JBoss_front1.png" imageanchor="1" style="clear: right; float: right; margin-bottom: 1em; margin-left: 1em;"&gt;&lt;img alt="effective business process management with jboss bpm" border="0" data-original-height="547" data-original-width="433" height="200" src="https://1.bp.blogspot.com/-ih_789OK1cM/WqEPoyQVptI/AAAAAAAAr7k/-FobkkL2a8Iv295xSqvKaUEatf2j1LxfwCPcBGAYYCw/s200/Schabell_JBoss_front1.png" title="" width="158" /&gt;&lt;/a&gt;Share your JBoss BPM story&lt;/h3&gt;It was wonderful to meet people from all across the globe. Just off the top of my head, I spoke with readers and JBoss BPM Suite fans from the UK, Australia, India, Japan, China, Canada, Mexico, Peru, Uruguay, Brazil and South Africa.&lt;br /&gt;&lt;br /&gt;There might have been more, but after two signing sessions of an hour, my mind was whirling from all the friends, colleagues, customers and partners that attended the signing table.&lt;br /&gt;&lt;br /&gt;I left the Red Hat Developers team with a few books pre-signed for their next event and they were kind enough to send me home with a small box of the book.&lt;br /&gt;&lt;br /&gt;&lt;a href="https://3.bp.blogspot.com/-cE_AwmnOPeI/Wv7BwOjdCQI/AAAAAAAAsqc/krGUibUNhvs6lmrXj70YWaUkKWGJATs8QCLcBGAs/s1600/IMG_9981.JPG" imageanchor="1" style="clear: right; float: right; margin-bottom: 1em; margin-left: 1em;"&gt;&lt;img alt="effective business process management with jboss bpm" border="0" data-original-height="1600" data-original-width="1200" height="200" src="https://3.bp.blogspot.com/-cE_AwmnOPeI/Wv7BwOjdCQI/AAAAAAAAsqc/krGUibUNhvs6lmrXj70YWaUkKWGJATs8QCLcBGAs/s200/IMG_9981.JPG" title="" width="150" /&gt;&lt;/a&gt;If you would like a free paper book copy, possibly signed by me the author, then here's what you have to do.&lt;br /&gt;&lt;br /&gt;Share your JBoss BPM story in a comments section of this post and let me know by &lt;a href="https://twitter.com/ericschabell" target="_blank"&gt;contacting me through my Twitter account&lt;/a&gt;.&lt;br /&gt;&lt;br /&gt;I'll be in touch about finding how to get a copy of the book, while the limited supply lasts, to you!&lt;br /&gt;&lt;br /&gt;&lt;/div&gt;&lt;div class="feedflare"&gt; &lt;a href="http://feeds.feedburner.com/~ff/schabell/jboss?a=Ia3YRhyx4jQ:8Sq0E4ixWEs:yIl2AUoC8zA"&gt;&lt;img src="http://feeds.feedburner.com/~ff/schabell/jboss?d=yIl2AUoC8zA" border="0"&gt;&lt;/img&gt;&lt;/a&gt; &lt;a href="http://feeds.feedburner.com/~ff/schabell/jboss?a=Ia3YRhyx4jQ:8Sq0E4ixWEs:63t7Ie-LG7Y"&gt;&lt;img src="http://feeds.feedburner.com/~ff/schabell/jboss?d=63t7Ie-LG7Y" border="0"&gt;&lt;/img&gt;&lt;/a&gt; &lt;a href="http://feeds.feedburner.com/~ff/schabell/jboss?a=Ia3YRhyx4jQ:8Sq0E4ixWEs:4cEx4HpKnUU"&gt;&lt;img src="http://feeds.feedburner.com/~ff/schabell/jboss?i=Ia3YRhyx4jQ:8Sq0E4ixWEs:4cEx4HpKnUU" border="0"&gt;&lt;/img&gt;&lt;/a&gt; &lt;a href="http://feeds.feedburner.com/~ff/schabell/jboss?a=Ia3YRhyx4jQ:8Sq0E4ixWEs:F7zBnMyn0Lo"&gt;&lt;img src="http://feeds.feedburner.com/~ff/schabell/jboss?i=Ia3YRhyx4jQ:8Sq0E4ixWEs:F7zBnMyn0Lo" border="0"&gt;&lt;/img&gt;&lt;/a&gt; &lt;a href="http://feeds.feedburner.com/~ff/schabell/jboss?a=Ia3YRhyx4jQ:8Sq0E4ixWEs:V_sGLiPBpWU"&gt;&lt;img src="http://feeds.feedburner.com/~ff/schabell/jboss?i=Ia3YRhyx4jQ:8Sq0E4ixWEs:V_sGLiPBpWU" border="0"&gt;&lt;/img&gt;&lt;/a&gt; &lt;a href="http://feeds.feedburner.com/~ff/schabell/jboss?a=Ia3YRhyx4jQ:8Sq0E4ixWEs:qj6IDK7rITs"&gt;&lt;img src="http://feeds.feedburner.com/~ff/schabell/jboss?d=qj6IDK7rITs" border="0"&gt;&lt;/img&gt;&lt;/a&gt; &lt;a href="http://feeds.feedburner.com/~ff/schabell/jboss?a=Ia3YRhyx4jQ:8Sq0E4ixWEs:gIN9vFwOqvQ"&gt;&lt;img src="http://feeds.feedburner.com/~ff/schabell/jboss?i=Ia3YRhyx4jQ:8Sq0E4ixWEs:gIN9vFwOqvQ" border="0"&gt;&lt;/img&gt;&lt;/a&gt; &lt;/div&gt;&lt;img src="http://feeds.feedburner.com/~r/schabell/jboss/~4/Ia3YRhyx4jQ" height="1" width="1" alt=""/&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/P_kEC_QYBEg" height="1" width="1" alt=""/&gt;</content><summary>As mentioned previously, a few weeks ago in San Francisco, CA I spent some time signing copies of my book Effective Business Process Management with JBoss BPM for attendees of the Red Hat Summit. Since the book was launched as a free ebook download on Red Hat Developers, it was not really possible to get your hands on a real paper copy of the book. Until now. With over 200 put in print for Red Hat...</summary><dc:creator>Eric D. Schabell</dc:creator><dc:date>2018-05-21T05:00:00Z</dc:date><feedburner:origLink>http://feedproxy.google.com/~r/schabell/jboss/~3/Ia3YRhyx4jQ/get-a-signed-copy-of-effective-business-process-management-with-jboss-bpm.html</feedburner:origLink></entry><entry><title>Red Hat Summit: An Eventful Tour from Enterprise Integration to Serverless</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/3w-cg_s4g0k/" /><category term="Design Patterns" scheme="searchisko:content:tags" /><category term="domain-driven design" scheme="searchisko:content:tags" /><category term="event-driven architecture" scheme="searchisko:content:tags" /><category term="FaaS" scheme="searchisko:content:tags" /><category term="feed_group_name_nonmiddleware" scheme="searchisko:content:tags" /><category term="feed_name_redhat_developer_blog" scheme="searchisko:content:tags" /><category term="Functions-as-a-Service" scheme="searchisko:content:tags" /><category term="integration" scheme="searchisko:content:tags" /><category term="microservices" scheme="searchisko:content:tags" /><category term="Modern App Dev" scheme="searchisko:content:tags" /><category term="red hat summit" scheme="searchisko:content:tags" /><category term="Red Hat Summit 2018" scheme="searchisko:content:tags" /><category term="serverless" scheme="searchisko:content:tags" /><author><name>unknown</name></author><id>searchisko:content:id:jbossorg_blog-red_hat_summit_an_eventful_tour_from_enterprise_integration_to_serverless</id><updated>2018-05-18T11:00:22Z</updated><published>2018-05-18T11:00:22Z</published><content type="html">&lt;p&gt;Red Hat Senior Architects Marius Bogoevici and Christian Posta recently presented an overview of event-driven architecture, taking the audience from the basics of enterprise integration to microservices and serverless computing. Standing in front of a packed room at Red Hat Summit, their talk addressed four basic points:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Event-driven architectures have been around for a while. What are they, why are they powerful, and why are back &lt;em&gt;en vogue&lt;/em&gt;?&lt;/li&gt; &lt;li&gt;Messaging is often used as a backbone for event-based distributed systems. What options do we have for cloud-native event-driven architectures?&lt;/li&gt; &lt;li&gt;Integration is necessary for any organization. How do streaming, cloud-native architectures, and microservices fit in?&lt;/li&gt; &lt;li&gt;Are Functions-as-a-Service (FaaS) the next utopian architecture? Where do functions fit in a world of microservices?&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;The entire session was done with three enterprise concerns in mind. First is the divide between agile systems and purpose-built systems. While the purpose-built system is optimized for a small set of use cases, it is very difficult to change if new use cases arise or the old use cases become irrelevant. We have to be agile to adapt to a constantly changing environment. Another concern is resource utilization. We want to eliminate waste and get the most out of our systems and resources, although the cloud in general and containers in particular make more distributed architectures practical. Finally, Christian made the observation that we cannot build complex systems from complex parts. The components we develop must be as simple and understandable as possible.&lt;/p&gt; &lt;p&gt;&lt;span id="more-494797"&gt;&lt;/span&gt;Marius explained the rise of event-driven architectures by comparing them to the old client-server paradigm. The most important differences are:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Client-server interactions are ephemeral and synchronous. Event-driven interactions are persistent (they can be, anyway) and asynchronous.&lt;/li&gt; &lt;li&gt;Client-server applications are tightly coupled. Event-driven applications are decoupled.&lt;/li&gt; &lt;li&gt;Client-server applications aren&amp;#8217;t easily composable. Event-driven applications are&lt;sup&gt;1&lt;/sup&gt;.&lt;/li&gt; &lt;li&gt;Client-server architectures have a simplified model (every request has a single response) and are not fault-tolerant. Event-driven architectures have a complicated model (a single event might be handled by several components) and are highly fault-tolerant.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;To start with the basics, an &lt;em&gt;event&lt;/em&gt; is simply an action or occurrence that happened in the past. It is immutable, it can be persistent, and it can be shared. Types of events are notifications, state transfers &lt;em&gt;(aka &lt;/em&gt;commands&lt;em&gt;),&lt;/em&gt; event sourcing, and &lt;a href="https://martinfowler.com/bliki/CQRS.html"&gt;CQRS (Command Query Responsibllity Segregation)&lt;/a&gt;. (See &lt;a href="https://martinfowler.com/articles/201701-event-driven.html"&gt;Martin Fowler&amp;#8217;s excellent article &amp;#8220;What do you mean by &amp;#8216;Event-Driven&amp;#8217;?&amp;#8221;&lt;/a&gt; for a great discussion on the subject.)&lt;/p&gt; &lt;p&gt;In an event-driven architecture, you treat events as part of your domain model and create decoupled components that either emit or handle those events. This dovetails with the concerns of &lt;a href="https://en.wikipedia.org/wiki/Domain-driven_design"&gt;domain-driven design&lt;/a&gt;. Marius used an example of a system with four components: Orders, Billing, Shipment, and Inventory. Those components deal with the events: Order Created, Order Paid, and Order Shipped. In this simple example, the interactions among the components become obvious. When the Orders component generates an Order Created event, the other three components are affected. The order impacts the inventory on hand, the customer must be billed for the order, and the order has to be shipped. By focusing on events, the behavior of the system is easy to understand.&lt;/p&gt; &lt;p&gt;Event-driven architecture leads to more agile systems. As mentioned earlier, composability makes it straightforward to add more components to the system. In addition, if the system uses persistent events, those events are available for data mining, analytics, and machine learning. All in all, event-driven architectures are more robust and resilient, they are agile, and they make it possible for the organization to align its digital business with what actually happens in the real world.&lt;/p&gt; &lt;p&gt;Of course, if you&amp;#8217;re going to build an event-driven architecture, you have to have an infrastructure that delivers a stream of events reliably. Middleware has evolved to address this. Traditional message brokers deliver functionality such as publish/subscribe, queueing, and persistence. In that infrastructure, all of the messages flow through the broker, creating a bottleneck. This is good from the perspective of system utilization, but it limits agility. New requirements have created orders of magnitude more events. For example, an application that tracks clicks of the Place Order button on a web page has a certain number of events. Tracking mouse movements on the page could give tremendous insights into user behavior, but it would create many more events. Messaging middleware has evolved to handle greater event volumes.&lt;/p&gt; &lt;p&gt;Systems like &lt;a href="http://kafka.apache.org"&gt;Apache Kafka&lt;/a&gt; decentralize the processing of messages to the individual services that are using them. That makes the system horizontally scalable, reduces the amount of coordination between parts of the broker infrastructure, and allows clients to come and go without impacting the broker. This simpler architecture is great for the collection and distribution of huge numbers of events at cloud scale. (To go beyond the basics of Kafka, take a look at &lt;a href="http://strimzi.io"&gt;Strimzi&lt;/a&gt;, a project to bring Kafka into the world of OpenShift and Kubernetes.)&lt;/p&gt; &lt;p&gt;Next, Marius turned to enterprise integration, starting with the characteristics of an Enterprise Service Bus:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;An ESB handles all the message traffic in the system, optimizing utilization but creating a bottleneck&lt;/li&gt; &lt;li&gt;An ESB is centralized and tightly coupled&lt;/li&gt; &lt;li&gt;An ESB mixes logic and infrastructure, including things like transformations and mediations with message delivery&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;In newer messaging frameworks like &lt;a href="http://camel.apache.org"&gt;Apache Camel&lt;/a&gt;, the responsibility for things like transformations are placed on the applications or components that handle the messages. This makes it possible to change application logic without reconfiguring a centralized component like an ESB. With the rise of cloud-native applications, the technology has evolved further. Marius used a diagram of a set of containerized Camel applications running in OpenShift, with services such as messaging being provided by the platform. He also pointed to Strimzi (Kafka as a service) and mentioned &lt;a href="http://enmasse.io"&gt;EnMasse&lt;/a&gt; (messaging as a service), both of which run inside OpenShift.&lt;/p&gt; &lt;p&gt;&lt;a href="http://www.enterpriseintegrationpatterns.com"&gt;Enterprise Integration Patterns&lt;/a&gt; were originally designed to build integrated systems out of siloed enterprise systems. The patterns are a good fit for distributed, event-driven systems, typically implemented by event-driven microservices. With today&amp;#8217;s message volumes, however, &lt;em&gt;streaming&lt;/em&gt; becomes a key design point. Event-driven applications need to view streams as continuous, unbounded flows of data (events), with those streams handled by small services working together. Data pipelines built on top of small services working together using frameworks like Camel or &lt;a href="https://kafka.apache.org/documentation/streams/"&gt;Kafka Streams&lt;/a&gt; can solve modern enterprise integration problems. This change in mindset is an adaptation to the agile, decentralized, cloud-native nature of modern event-driven systems.&lt;/p&gt; &lt;p&gt;Which brings us to microservices themselves. A well-designed microservice has a specific business function and can be deployed and developed independently from other microservices. This enables agility and allows multiple development teams to work in parallel. Microservices are frequently containerized to increase density and utilization and reduce the overhead of running multiple services. Although the design concepts behind microservices have been around for years, the combination of cloud architectures and containerization has made them the obvious choice for many applications.&lt;/p&gt; &lt;p&gt;But not all. Christian made the important point that while microservices are great at enabling agility from existing systems, you shouldn&amp;#8217;t optimize your applications for microservices unless you have problems with your current architecture. Specifically, &lt;em&gt;if agility isn&amp;#8217;t the problem with your existing system, microservices are not the solution&lt;/em&gt;. As an example, he mentioned an HR system only used on the last day of each week. On that one day, utilization and compute requirements are high, but only on that day. It doesn&amp;#8217;t make sense to have a set of services around to serve that traffic constantly when you know that traffic isn&amp;#8217;t constant. The main message: &lt;em&gt;Understand your use cases.&lt;/em&gt;&lt;/p&gt; &lt;p&gt;Continuing the discussion of integration use cases, tasks like webhook callbacks, scheduled tasks, file processing, and reacting to database changes are better suited to the FaaS model. The code in a FaaS system is run by the platform whenever certain events happen. The task of the development team is to write the code that handles each event and the rules that define when the code should be invoked. As a result, the system has high utilization and parallelization, and the resources needed to handle those events are managed by the FaaS provider.&lt;/p&gt; &lt;p&gt;There are four options to consider as you build cloud-native applications:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Event-driven microservices&lt;/li&gt; &lt;li&gt;Containers&lt;/li&gt; &lt;li&gt;FaaS&lt;/li&gt; &lt;li&gt;Other serverless components such as databases, message queues, and caches&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Marius and Christian both made the point that all of these technologies have their place. Despite the current hype cycle, not everything is a good candidate for FaaS. Again, it comes down to your use case. If you have well-understood boundaries and you know what those boundaries are, microservices could be the answer. On the other hand, if you have an exploratory use case and you don&amp;#8217;t know its traffic patterns and utilization (and might not even know if the use case provides any business value at all), FaaS could let you experiment without a lot of overhead.&lt;/p&gt; &lt;p&gt;This was a great session with lots of insight from two highly experienced architects. If this post whetted your appetite, we encourage you to watch &lt;a href="https://youtu.be/jImAwSRehRs"&gt;the video recording of their presentation&lt;/a&gt;.&lt;/p&gt; &lt;div align="center"&gt;&lt;iframe src="https://www.youtube.com/embed/jImAwSRehRs" width="560" height="315" frameborder="0" allowfullscreen="allowfullscreen"&gt;&lt;span data-mce-type="bookmark" style="display: inline-block; width: 0px; overflow: hidden; line-height: 0;" class="mce_SELRES_start"&gt;﻿&lt;/span&gt;&lt;/iframe&gt;&lt;/div&gt; &lt;p&gt;&amp;#160;&lt;/p&gt; &lt;hr /&gt; &lt;p&gt;&lt;sup&gt;1&lt;/sup&gt; If System A and System B are client and server, composing a new application that adds System C is difficult because that almost certainly requires changes to Systems A and B. In the decoupled world of event-driven applications, introducing Systems C, D, and E shouldn&amp;#8217;t require any changes to Systems A and B. In fact, it&amp;#8217;s quite likely that A, B, C, D, and E have absolutely no knowledge of each other.&lt;/p&gt; &lt;p&gt;&lt;a class="a2a_button_facebook" href="https://www.addtoany.com/add_to/facebook?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2018%2F05%2F18%2Feventful-tour-enterprise-integration-to-serverless%2F&amp;#38;linkname=Red%20Hat%20Summit%3A%20An%20Eventful%20Tour%20from%20Enterprise%20Integration%20to%20Serverless" title="Facebook" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_twitter" href="https://www.addtoany.com/add_to/twitter?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2018%2F05%2F18%2Feventful-tour-enterprise-integration-to-serverless%2F&amp;#38;linkname=Red%20Hat%20Summit%3A%20An%20Eventful%20Tour%20from%20Enterprise%20Integration%20to%20Serverless" title="Twitter" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_google_plus" href="https://www.addtoany.com/add_to/google_plus?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2018%2F05%2F18%2Feventful-tour-enterprise-integration-to-serverless%2F&amp;#38;linkname=Red%20Hat%20Summit%3A%20An%20Eventful%20Tour%20from%20Enterprise%20Integration%20to%20Serverless" title="Google+" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_linkedin" href="https://www.addtoany.com/add_to/linkedin?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2018%2F05%2F18%2Feventful-tour-enterprise-integration-to-serverless%2F&amp;#38;linkname=Red%20Hat%20Summit%3A%20An%20Eventful%20Tour%20from%20Enterprise%20Integration%20to%20Serverless" title="LinkedIn" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_email" href="https://www.addtoany.com/add_to/email?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2018%2F05%2F18%2Feventful-tour-enterprise-integration-to-serverless%2F&amp;#38;linkname=Red%20Hat%20Summit%3A%20An%20Eventful%20Tour%20from%20Enterprise%20Integration%20to%20Serverless" title="Email" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_print" href="https://www.addtoany.com/add_to/print?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2018%2F05%2F18%2Feventful-tour-enterprise-integration-to-serverless%2F&amp;#38;linkname=Red%20Hat%20Summit%3A%20An%20Eventful%20Tour%20from%20Enterprise%20Integration%20to%20Serverless" title="Print" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_reddit" href="https://www.addtoany.com/add_to/reddit?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2018%2F05%2F18%2Feventful-tour-enterprise-integration-to-serverless%2F&amp;#38;linkname=Red%20Hat%20Summit%3A%20An%20Eventful%20Tour%20from%20Enterprise%20Integration%20to%20Serverless" title="Reddit" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_flipboard" href="https://www.addtoany.com/add_to/flipboard?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2018%2F05%2F18%2Feventful-tour-enterprise-integration-to-serverless%2F&amp;#38;linkname=Red%20Hat%20Summit%3A%20An%20Eventful%20Tour%20from%20Enterprise%20Integration%20to%20Serverless" title="Flipboard" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_dd addtoany_share_save addtoany_share" href="https://www.addtoany.com/share#url=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2018%2F05%2F18%2Feventful-tour-enterprise-integration-to-serverless%2F&amp;#38;title=Red%20Hat%20Summit%3A%20An%20Eventful%20Tour%20from%20Enterprise%20Integration%20to%20Serverless" data-a2a-url="https://developers.redhat.com/blog/2018/05/18/eventful-tour-enterprise-integration-to-serverless/" data-a2a-title="Red Hat Summit: An Eventful Tour from Enterprise Integration to Serverless"&gt;&lt;img src="https://static.addtoany.com/buttons/favicon.png" alt="Share"&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;The post &lt;a rel="nofollow" href="https://developers.redhat.com/blog/2018/05/18/eventful-tour-enterprise-integration-to-serverless/"&gt;Red Hat Summit: An Eventful Tour from Enterprise Integration to Serverless&lt;/a&gt; appeared first on &lt;a rel="nofollow" href="https://developers.redhat.com/blog"&gt;RHD Blog&lt;/a&gt;.&lt;/p&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/3w-cg_s4g0k" height="1" width="1" alt=""/&gt;</content><summary>Red Hat Senior Architects Marius Bogoevici and Christian Posta recently presented an overview of event-driven architecture, taking the audience from the basics of enterprise integration to microservices and serverless computing. Standing in front of a packed room at Red Hat Summit, their talk addressed four basic points: Event-driven architectures have been around for a while. What are they, why a...</summary><dc:creator>unknown</dc:creator><dc:date>2018-05-18T11:00:22Z</dc:date><feedburner:origLink>https://developers.redhat.com/blog/2018/05/18/eventful-tour-enterprise-integration-to-serverless/</feedburner:origLink></entry></feed>
